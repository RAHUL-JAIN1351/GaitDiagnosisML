{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import linear_model, ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import find_peaks_cwt\n",
    "from FeatureAnalysis import RescalePCA, HistFeature, FFTFeature\n",
    "from DataProcessing import ToPandasData, DataCombiner, DataAugmentor\n",
    "from DataVisulization import VisCompareTwoPCA, VisMeasurements, VisFFTResult, VisTrainVsTest, VisPredResult\n",
    "import random\n",
    "import time\n",
    "# keras\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COMBINER = DataCombiner()\n",
    "# convert all trial data and to be ready to pass in to the PCA \n",
    "# structure: {gait_type:{trial_name: total_trial_num}}\n",
    "All_gait_trials = {'normal': ('Normal', 5), 'normal_2': ('Normal',5), 'normal_3':('Normal', 6),'ran':('Normal', 3),\n",
    "                   'side':('Side Walk', 5), 'side_2':('Side Walk', 5),'ran_side':('Side Walk', 5),\n",
    "                   'inward_01': ('Inward', 5),\n",
    "                   'line_01':('Line', 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Loading: From txt files into Pandas\n",
      "Finish Loading: Time taken 1.604 sec\n",
      "\n",
      "Start Combining: Combine specific type trials under each general type\n",
      "Finish Combining: Time taken 0.036 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_COMBINER.loadTrialData(All_gait_trials, '../Data_Collection/','.txt')\n",
    "All_combined_data = DATA_COMBINER.combineAllData()\n",
    "gait_types = DATA_COMBINER.label_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Inward': {'inward_01':        Voltage_01  Voltage_02  Acc_x  Acc_y  Acc_z  Gyro_x  Gyro_y  Gyro_z  \\\n",
       "  0        0.333333    0.229630   0.83  -0.70   9.92   -0.08    0.08    0.06   \n",
       "  1        0.291667    0.222222   0.75  -0.69   9.99   -0.07    0.07    0.06   \n",
       "  2        0.291667    0.303704   0.73  -0.68  10.04   -0.07    0.07    0.07   \n",
       "  3        0.333333    0.340741   0.71  -0.65  10.03   -0.06    0.03    0.07   \n",
       "  4        0.291667    0.303704   0.76  -0.63  10.07   -0.05    0.07    0.07   \n",
       "  5        0.250000    0.340741   0.74  -0.58  10.07   -0.03    0.07    0.07   \n",
       "  6        0.208333    0.362963   0.77  -0.62   9.99   -0.02    0.07    0.07   \n",
       "  7        0.250000    0.340741   0.79  -0.60  10.07   -0.01    0.07    0.07   \n",
       "  8        0.333333    0.340741   0.80  -0.59   9.99   -0.00    0.07    0.07   \n",
       "  9        0.291667    0.348148   0.78  -0.63  10.01    0.00    0.07    0.06   \n",
       "  10       0.250000    0.340741   0.83  -0.64   9.96    0.01    0.06    0.07   \n",
       "  11       0.208333    0.333333   0.73  -0.69   9.99    0.01    0.06    0.07   \n",
       "  12       0.250000    0.318519   0.77  -0.68   9.97    0.01    0.06    0.06   \n",
       "  13       0.291667    0.311111   0.78  -0.69   9.98    0.01    0.06    0.06   \n",
       "  14       0.291667    0.311111   0.80  -0.69  10.00    0.01    0.05    0.06   \n",
       "  15       0.291667    0.281481   0.80  -0.68  10.01    0.01    0.05    0.06   \n",
       "  16       0.250000    0.259259   0.81  -0.70   9.99    0.01    0.04    0.06   \n",
       "  17       0.291667    0.229630   0.79  -0.68   9.93    0.01    0.04    0.06   \n",
       "  18       0.333333    0.192593   0.81  -0.68  10.00    0.01    0.00    0.06   \n",
       "  19       0.291667    0.185185   0.83  -0.66  10.00    0.01    0.03    0.06   \n",
       "  20       0.250000    0.162963   0.85  -0.70   9.98    0.01    0.03    0.06   \n",
       "  21       0.333333    0.148148   0.89  -0.69  10.00    0.01    0.02    0.06   \n",
       "  22       0.375000    0.148148   0.83  -0.70   9.97    0.02    0.02    0.05   \n",
       "  23       0.333333    0.140741   0.82  -0.69   9.97    0.01    0.02    0.06   \n",
       "  24       0.375000    0.111111   0.89  -0.74   9.94    0.01    0.02    0.06   \n",
       "  25       0.333333    0.103704   0.91  -0.70   9.99    0.01    0.02    0.05   \n",
       "  26       0.333333    0.096296   0.88  -0.68  10.04    0.01    0.02    0.05   \n",
       "  27       0.333333    0.118519   0.90  -0.71   9.93    0.00    0.01    0.05   \n",
       "  28       0.333333    0.096296   0.88  -0.70   9.98    0.00    0.02    0.05   \n",
       "  29       0.333333    0.088889   0.89  -0.69  10.02    0.00    0.02    0.05   \n",
       "  ...           ...         ...    ...    ...    ...     ...     ...     ...   \n",
       "  24970    0.230769    0.043478   1.31  -0.98  10.01    0.00   -0.03   -0.07   \n",
       "  24971    0.192308    0.043478   1.34  -0.92   9.98    0.01   -0.02   -0.08   \n",
       "  24972    0.230769    0.043478   1.35  -0.93  10.01    0.02   -0.02   -0.09   \n",
       "  24973    0.230769    0.028986   1.21  -1.00  10.12    0.04   -0.01   -0.09   \n",
       "  24974    0.230769    0.028986   1.16  -1.00  10.06    0.07    0.00   -0.09   \n",
       "  24975    0.230769    0.043478   1.11  -1.01  10.02    0.08    0.01   -0.08   \n",
       "  24976    0.230769    0.043478   1.09  -1.09   9.97    0.09    0.02   -0.07   \n",
       "  24977    0.230769    0.028986   1.08  -1.08   9.98    0.10    0.03   -0.05   \n",
       "  24978    0.230769    0.028986   1.26  -1.08  10.04    0.10    0.03   -0.04   \n",
       "  24979    0.230769    0.043478   1.22  -1.11   9.98    0.12    0.02   -0.03   \n",
       "  24980    0.230769    0.028986   1.31  -1.06   9.88    0.11    0.03   -0.03   \n",
       "  24981    0.269231    0.028986   1.29  -1.08   9.91    0.11    0.03   -0.03   \n",
       "  24982    0.269231    0.028986   1.31  -1.08   9.95    0.09    0.03   -0.03   \n",
       "  24983    0.269231    0.028986   1.35  -1.12   9.88    0.06    0.01   -0.03   \n",
       "  24984    0.269231    0.028986   1.38  -1.08   9.86    0.04   -0.02   -0.03   \n",
       "  24985    0.269231    0.028986   1.41  -1.10   9.83    0.00   -0.04   -0.04   \n",
       "  24986    0.269231    0.028986   1.45  -1.13   9.82   -0.03   -0.06   -0.05   \n",
       "  24987    0.307692    0.028986   1.46  -1.13   9.82   -0.07   -0.09   -0.06   \n",
       "  24988    0.269231    0.028986   1.50  -1.10   9.91   -0.09   -0.10   -0.08   \n",
       "  24989    0.269231    0.028986   1.46  -1.06   9.92   -0.10   -0.11   -0.09   \n",
       "  24990    0.230769    0.028986   1.42  -1.05   9.92   -0.10   -0.10   -0.11   \n",
       "  24991    0.230769    0.043478   1.37  -1.07   9.96   -0.09   -0.08   -0.12   \n",
       "  24992    0.230769    0.043478   1.39  -1.10  10.04   -0.09   -0.07   -0.13   \n",
       "  24993    0.230769    0.043478   1.31  -1.10  10.08   -0.06   -0.05   -0.13   \n",
       "  24994    0.230769    0.028986   1.17  -1.08  10.12   -0.05   -0.03   -0.12   \n",
       "  24995    0.230769    0.028986   1.17  -1.12  10.18   -0.03   -0.01   -0.10   \n",
       "  24996    0.269231    0.028986   1.06  -1.07  10.17   -0.01    0.00   -0.08   \n",
       "  24997    0.230769    0.028986   1.05  -1.06  10.21    0.01    0.01   -0.04   \n",
       "  24998    0.230769    0.043478   1.20  -1.07  10.24    0.03    0.01   -0.03   \n",
       "  24999    0.269231    0.028986   1.20  -1.09  10.27    0.08    0.01   -0.01   \n",
       "  \n",
       "              Time  \n",
       "  0        0.01700  \n",
       "  1        0.02300  \n",
       "  2        0.02900  \n",
       "  3        0.03500  \n",
       "  4        0.04100  \n",
       "  5        0.04800  \n",
       "  6        0.05400  \n",
       "  7        0.06000  \n",
       "  8        0.06600  \n",
       "  9        0.07200  \n",
       "  10       0.07800  \n",
       "  11       0.08400  \n",
       "  12       0.09100  \n",
       "  13       0.09700  \n",
       "  14       0.10300  \n",
       "  15       0.11000  \n",
       "  16       0.11600  \n",
       "  17       0.12200  \n",
       "  18       0.12900  \n",
       "  19       0.13500  \n",
       "  20       0.14100  \n",
       "  21       0.14700  \n",
       "  22       0.15300  \n",
       "  23       0.15900  \n",
       "  24       0.16500  \n",
       "  25       0.17200  \n",
       "  26       0.17800  \n",
       "  27       0.18400  \n",
       "  28       0.19100  \n",
       "  29       0.19700  \n",
       "  ...          ...  \n",
       "  24970  157.89128  \n",
       "  24971  157.89728  \n",
       "  24972  157.90328  \n",
       "  24973  157.91028  \n",
       "  24974  157.91628  \n",
       "  24975  157.92328  \n",
       "  24976  157.92928  \n",
       "  24977  157.93528  \n",
       "  24978  157.94128  \n",
       "  24979  157.94728  \n",
       "  24980  157.95428  \n",
       "  24981  157.96128  \n",
       "  24982  157.96728  \n",
       "  24983  157.97328  \n",
       "  24984  157.97928  \n",
       "  24985  157.98528  \n",
       "  24986  157.99128  \n",
       "  24987  157.99828  \n",
       "  24988  158.00528  \n",
       "  24989  158.01128  \n",
       "  24990  158.01728  \n",
       "  24991  158.02328  \n",
       "  24992  158.02928  \n",
       "  24993  158.03628  \n",
       "  24994  158.04228  \n",
       "  24995  158.04928  \n",
       "  24996  158.05528  \n",
       "  24997  158.06128  \n",
       "  24998  158.06728  \n",
       "  24999  158.07428  \n",
       "  \n",
       "  [25000 rows x 9 columns]},\n",
       " 'Line': {'line_01':        Voltage_01  Voltage_02  Acc_x  Acc_y  Acc_z  Gyro_x  Gyro_y  Gyro_z  \\\n",
       "  0        0.076923    0.045045   1.67  -3.89  16.07    3.45    3.19    0.09   \n",
       "  1        0.059829    0.045045   2.47  -4.05  16.57    3.66    3.47    0.16   \n",
       "  2        0.085470    0.054054   3.77  -4.89  18.43    3.81    3.88    0.22   \n",
       "  3        0.076923    0.045045   5.13  -5.41  19.60    3.91    4.27    0.28   \n",
       "  4        0.076923    0.045045   6.05  -5.80  20.56    3.98    4.28    0.35   \n",
       "  5        0.068376    0.045045   7.82  -6.43  21.90    4.02    4.28    0.42   \n",
       "  6        0.051282    0.054054   9.51  -6.89  23.43    3.95    4.28    0.51   \n",
       "  7        0.068376    0.063063  11.60  -6.81  24.26    3.80    4.28    0.52   \n",
       "  8        0.059829    0.045045  10.65  -5.96  26.27    3.62    4.28    0.35   \n",
       "  9        0.059829    0.054054   4.54  -4.63  27.76    3.43    4.28   -0.04   \n",
       "  10       0.051282    0.054054  -0.88  -3.75  26.35    3.47    4.28   -0.42   \n",
       "  11       0.059829    0.063063  -5.56  -2.11  22.23    3.71    4.28   -0.68   \n",
       "  12       0.059829    0.045045  -7.15   0.23  18.28    3.93    4.28   -0.92   \n",
       "  13       0.051282    0.054054  -6.69   3.66  12.88    3.89    4.28   -1.06   \n",
       "  14       0.042735    0.054054  -6.36   6.57   9.13    3.55    4.17   -1.13   \n",
       "  15       0.051282    0.054054  -4.58   7.78   9.06    3.08    2.89   -1.17   \n",
       "  16       0.051282    0.054054 -11.62  11.88   3.67    2.77    1.41   -1.54   \n",
       "  17       0.051282    0.054054 -10.86  11.00  24.53    1.75   -0.91   -2.36   \n",
       "  18       0.034188    0.054054  -3.54  13.14  19.77   -0.64   -1.76   -2.36   \n",
       "  19       0.051282    0.054054   2.95  12.41  17.25   -1.93   -2.68   -1.90   \n",
       "  20       0.042735    0.054054   0.39  10.04  21.00   -2.56   -3.30   -1.59   \n",
       "  21       0.034188    0.054054  -1.14   8.01  17.75   -3.14   -3.12   -1.87   \n",
       "  22       0.042735    0.054054  -1.13   6.17  13.14   -3.43   -3.12   -2.36   \n",
       "  23       0.059829    0.054054  -1.05   5.87  10.70   -3.61   -3.22   -2.73   \n",
       "  24       0.051282    0.054054  -1.23   5.97   8.82   -3.69   -3.28   -3.01   \n",
       "  25       0.051282    0.054054  -0.90   5.70   8.66   -3.69   -3.32   -3.17   \n",
       "  26       0.034188    0.054054  -0.16   5.12   8.43   -3.65   -3.37   -3.36   \n",
       "  27       0.042735    0.054054   0.41   4.59   7.35   -3.58   -3.35   -3.49   \n",
       "  28       0.034188    0.054054   1.66   4.11   6.34   -3.54   -3.14   -3.60   \n",
       "  29       0.042735    0.054054   2.74   4.03   6.00   -3.45   -2.95   -3.70   \n",
       "  ...           ...         ...    ...    ...    ...     ...     ...     ...   \n",
       "  24970    0.034014    0.605263   0.31   0.06  10.93    0.42   -0.99    0.07   \n",
       "  24971    0.027211    0.611842   1.05  -0.56  10.48    0.29   -0.79    0.02   \n",
       "  24972    0.013605    0.611842   1.38  -0.68  10.22    0.19   -0.60    0.04   \n",
       "  24973    0.034014    0.644737   1.43  -1.12  10.99    0.15   -0.51    0.04   \n",
       "  24974    0.047619    0.644737   1.21  -0.37  10.17    0.12   -0.48    0.03   \n",
       "  24975    0.034014    0.644737   1.04  -0.82   9.92    0.10   -0.44    0.03   \n",
       "  24976    0.034014    0.625000   0.85  -0.92  10.44    0.07   -0.33    0.03   \n",
       "  24977    0.034014    0.625000   0.77  -0.98  10.46    0.03   -0.24    0.03   \n",
       "  24978    0.034014    0.631579   0.91  -0.86  10.15    0.02   -0.19    0.03   \n",
       "  24979    0.040816    0.644737   0.92  -0.86  10.09    0.02   -0.17    0.04   \n",
       "  24980    0.040816    0.638158   0.97  -0.73  10.18    0.02   -0.14    0.04   \n",
       "  24981    0.034014    0.598684   0.91  -0.75  10.07    0.03   -0.12    0.04   \n",
       "  24982    0.040816    0.572368   0.95  -0.79   9.99    0.03   -0.11    0.05   \n",
       "  24983    0.040816    0.559211   0.94  -0.80  10.01    0.03   -0.11    0.05   \n",
       "  24984    0.040816    0.526316   0.97  -0.79   9.99    0.03   -0.10    0.05   \n",
       "  24985    0.054422    0.486842   0.96  -0.80   9.98    0.03   -0.11    0.05   \n",
       "  24986    0.054422    0.467105   1.03  -0.75   9.92    0.03   -0.11    0.04   \n",
       "  24987    0.061224    0.440789   1.03  -0.70   9.96    0.02   -0.12    0.04   \n",
       "  24988    0.061224    0.421053   0.99  -0.72   9.95    0.01   -0.12    0.03   \n",
       "  24989    0.061224    0.447368   1.00  -0.75   9.91    0.01   -0.11    0.03   \n",
       "  24990    0.068027    0.414474   0.96  -0.77   9.93    0.01   -0.11    0.02   \n",
       "  24991    0.068027    0.388158   0.97  -0.72   9.98    0.00   -0.10    0.02   \n",
       "  24992    0.068027    0.375000   0.95  -0.75   9.95    0.01   -0.10    0.02   \n",
       "  24993    0.068027    0.348684   0.98  -0.75  10.02    0.01   -0.10    0.02   \n",
       "  24994    0.061224    0.322368   0.94  -0.78   9.83    0.01   -0.10    0.02   \n",
       "  24995    0.054422    0.322368   0.94  -0.80   9.99    0.01   -0.10    0.02   \n",
       "  24996    0.061224    0.309211   0.94  -0.78   9.97    0.01   -0.10    0.02   \n",
       "  24997    0.061224    0.269737   0.95  -0.79  10.03    0.01   -0.11    0.02   \n",
       "  24998    0.074830    0.243421   0.89  -0.74   9.96    0.01   -0.11    0.03   \n",
       "  24999    0.074830    0.236842   0.90  -0.78  10.01    0.01   -0.11    0.02   \n",
       "  \n",
       "               Time  \n",
       "  0        0.017000  \n",
       "  1        0.023000  \n",
       "  2        0.029000  \n",
       "  3        0.035000  \n",
       "  4        0.041000  \n",
       "  5        0.048000  \n",
       "  6        0.054000  \n",
       "  7        0.060000  \n",
       "  8        0.066000  \n",
       "  9        0.072000  \n",
       "  10       0.078000  \n",
       "  11       0.086000  \n",
       "  12       0.092000  \n",
       "  13       0.098000  \n",
       "  14       0.104000  \n",
       "  15       0.110000  \n",
       "  16       0.116000  \n",
       "  17       0.122000  \n",
       "  18       0.129000  \n",
       "  19       0.136000  \n",
       "  20       0.142000  \n",
       "  21       0.148000  \n",
       "  22       0.154000  \n",
       "  23       0.160000  \n",
       "  24       0.166000  \n",
       "  25       0.173000  \n",
       "  26       0.179000  \n",
       "  27       0.185000  \n",
       "  28       0.191000  \n",
       "  29       0.197000  \n",
       "  ...           ...  \n",
       "  24970  158.799425  \n",
       "  24971  158.805425  \n",
       "  24972  158.811425  \n",
       "  24973  158.817425  \n",
       "  24974  158.825425  \n",
       "  24975  158.831425  \n",
       "  24976  158.837425  \n",
       "  24977  158.843425  \n",
       "  24978  158.850425  \n",
       "  24979  158.856425  \n",
       "  24980  158.863425  \n",
       "  24981  158.869425  \n",
       "  24982  158.876425  \n",
       "  24983  158.882425  \n",
       "  24984  158.888425  \n",
       "  24985  158.894425  \n",
       "  24986  158.900425  \n",
       "  24987  158.908425  \n",
       "  24988  158.914425  \n",
       "  24989  158.920425  \n",
       "  24990  158.926425  \n",
       "  24991  158.932425  \n",
       "  24992  158.938425  \n",
       "  24993  158.945425  \n",
       "  24994  158.952425  \n",
       "  24995  158.958425  \n",
       "  24996  158.964425  \n",
       "  24997  158.970425  \n",
       "  24998  158.977425  \n",
       "  24999  158.983425  \n",
       "  \n",
       "  [25000 rows x 9 columns]},\n",
       " 'Normal': {'normal':        Voltage_01  Voltage_02  Acc_x  Acc_y  Acc_z  Gyro_x  Gyro_y  Gyro_z  \\\n",
       "  0        0.333333    0.027027   0.78  -1.68  11.79    1.90    0.01   -0.20   \n",
       "  1        0.333333    0.036036   0.78  -1.81  11.75    1.96   -0.03   -0.20   \n",
       "  2        0.352201    0.036036   0.89  -1.93  11.86    2.08   -0.06   -0.19   \n",
       "  3        0.364780    0.036036   0.52  -1.87  11.39    2.16   -0.08   -0.18   \n",
       "  4        0.364780    0.027027   0.33  -1.95  11.13    2.24   -0.11   -0.16   \n",
       "  5        0.352201    0.045045   0.86  -2.38  11.66    2.35   -0.09   -0.15   \n",
       "  6        0.333333    0.036036   0.40  -2.47  11.42    2.45   -0.06   -0.14   \n",
       "  7        0.327044    0.036036   0.70  -2.94  11.68    2.59   -0.01   -0.12   \n",
       "  8        0.301887    0.045045   0.68  -2.98  12.31    2.75    0.06   -0.09   \n",
       "  9        0.301887    0.036036   1.20  -3.18  12.57    2.98    0.11   -0.09   \n",
       "  10       0.232704    0.036036   1.17  -3.28  12.90    3.20    0.14   -0.12   \n",
       "  11       0.201258    0.027027   0.94  -3.91  13.77    3.40    0.16   -0.15   \n",
       "  12       0.188679    0.036036   1.02  -4.34  14.09    3.65    0.19   -0.15   \n",
       "  13       0.169811    0.036036   0.98  -4.95  14.74    3.86    0.23   -0.15   \n",
       "  14       0.144654    0.045045   1.15  -5.62  16.60    4.07    0.27   -0.15   \n",
       "  15       0.144654    0.036036   1.53  -6.51  18.23    4.20    0.34   -0.14   \n",
       "  16       0.132075    0.045045   1.91  -6.07  16.54    4.26    0.42   -0.15   \n",
       "  17       0.125786    0.045045   2.22  -7.08  19.10    4.28    0.52   -0.17   \n",
       "  18       0.125786    0.045045   1.78  -6.89  18.97    4.28    0.67   -0.19   \n",
       "  19       0.113208    0.045045   1.17  -6.59  18.80    4.28    0.75   -0.22   \n",
       "  20       0.100629    0.045045   0.72  -5.83  18.57    4.28    0.80   -0.24   \n",
       "  21       0.113208    0.045045   0.12  -5.78  18.02    4.28    0.78   -0.25   \n",
       "  22       0.081761    0.045045  -0.18  -5.83  18.07    4.28    0.75   -0.24   \n",
       "  23       0.069182    0.054054  -0.37  -5.79  20.41    4.28    0.66   -0.23   \n",
       "  24       0.056604    0.054054  -0.34  -6.04  24.87    4.28    0.38   -0.24   \n",
       "  25       0.056604    0.054054   0.91  -5.87  23.93    4.28    0.09   -0.26   \n",
       "  26       0.050314    0.054054   3.11  -2.97  20.70    4.28   -0.14   -0.27   \n",
       "  27       0.044025    0.054054   4.95  -0.47  13.59    4.28   -0.09   -0.28   \n",
       "  28       0.044025    0.054054   6.64   3.71  10.69    3.77    0.23   -0.27   \n",
       "  29       0.044025    0.054054   7.47  10.62   7.56    3.07    0.73   -0.43   \n",
       "  ...           ...         ...    ...    ...    ...     ...     ...     ...   \n",
       "  24969    0.040698    0.060870   0.51  -0.93   9.83    0.08    0.05   -0.13   \n",
       "  24970    0.040698    0.060870   0.49  -0.91   9.82    0.10    0.05   -0.13   \n",
       "  24971    0.040698    0.060870   0.49  -0.90   9.87    0.12    0.05   -0.14   \n",
       "  24972    0.040698    0.060870   0.53  -0.90   9.85    0.15    0.05   -0.14   \n",
       "  24973    0.040698    0.060870   0.52  -1.02   9.96    0.19    0.05   -0.15   \n",
       "  24974    0.040698    0.060870   0.46  -1.08   9.99    0.24    0.06   -0.16   \n",
       "  24975    0.040698    0.060870   0.51  -1.04  10.03    0.32    0.06   -0.17   \n",
       "  24976    0.040698    0.060870   0.50  -1.18  10.16    0.38    0.06   -0.18   \n",
       "  24977    0.040698    0.060870   0.42  -1.16  10.28    0.50    0.05   -0.18   \n",
       "  24978    0.040698    0.060870   0.46  -1.22  10.68    0.63    0.06   -0.19   \n",
       "  24979    0.040698    0.060870   0.54  -1.23  11.12    0.81    0.06   -0.19   \n",
       "  24980    0.040698    0.060870   0.51  -1.35  11.24    0.99    0.05   -0.20   \n",
       "  24981    0.040698    0.060870   0.11  -1.44  11.20    1.13    0.02   -0.20   \n",
       "  24982    0.040698    0.060870   0.56  -1.49  11.75    1.28   -0.03   -0.21   \n",
       "  24983    0.040698    0.060870   0.58  -1.60  11.84    1.45   -0.13   -0.22   \n",
       "  24984    0.040698    0.060870   0.44  -1.76  12.15    1.60   -0.20   -0.21   \n",
       "  24985    0.040698    0.060870   0.42  -1.77  11.90    1.77   -0.26   -0.21   \n",
       "  24986    0.040698    0.060870   0.25  -1.75  12.54    1.93   -0.30   -0.21   \n",
       "  24987    0.040698    0.060870   0.27  -1.96  12.71    2.13   -0.30   -0.20   \n",
       "  24988    0.040698    0.060870   0.09  -1.94  12.93    2.38   -0.33   -0.20   \n",
       "  24989    0.040698    0.060870   0.20  -2.16  12.78    2.60   -0.35   -0.18   \n",
       "  24990    0.040698    0.060870   0.33  -2.36  12.76    2.85   -0.36   -0.17   \n",
       "  24991    0.040698    0.060870   0.32  -2.51  13.32    3.08   -0.43   -0.18   \n",
       "  24992    0.040698    0.060870   0.67  -2.87  14.07    3.39   -0.46   -0.20   \n",
       "  24993    0.040698    0.060870  -0.50  -2.59  17.08    3.71   -0.56   -0.22   \n",
       "  24994    0.040698    0.060870   0.29  -2.87  13.79    4.02   -0.52   -0.24   \n",
       "  24995    0.040698    0.060870   0.04  -3.13  15.68    4.25   -0.57   -0.23   \n",
       "  24996    0.040698    0.060870   1.28  -3.51  16.26    4.28   -0.68   -0.26   \n",
       "  24997    0.040698    0.060870   1.69  -3.85  16.71    4.28   -0.82   -0.40   \n",
       "  24998    0.040698    0.060870   2.34  -4.78  17.55    4.28   -0.90   -0.55   \n",
       "  \n",
       "               Time  \n",
       "  0        0.024000  \n",
       "  1        0.032000  \n",
       "  2        0.039000  \n",
       "  3        0.047000  \n",
       "  4        0.054000  \n",
       "  5        0.061000  \n",
       "  6        0.068000  \n",
       "  7        0.075000  \n",
       "  8        0.082000  \n",
       "  9        0.090000  \n",
       "  10       0.097000  \n",
       "  11       0.103000  \n",
       "  12       0.111000  \n",
       "  13       0.118000  \n",
       "  14       0.125000  \n",
       "  15       0.132000  \n",
       "  16       0.139000  \n",
       "  17       0.146000  \n",
       "  18       0.153000  \n",
       "  19       0.160000  \n",
       "  20       0.167000  \n",
       "  21       0.175000  \n",
       "  22       0.182000  \n",
       "  23       0.189000  \n",
       "  24       0.196000  \n",
       "  25       0.203000  \n",
       "  26       0.210000  \n",
       "  27       0.218000  \n",
       "  28       0.225000  \n",
       "  29       0.232000  \n",
       "  ...           ...  \n",
       "  24969  179.025661  \n",
       "  24970  179.032661  \n",
       "  24971  179.039661  \n",
       "  24972  179.046661  \n",
       "  24973  179.053661  \n",
       "  24974  179.060661  \n",
       "  24975  179.068661  \n",
       "  24976  179.075661  \n",
       "  24977  179.082661  \n",
       "  24978  179.089661  \n",
       "  24979  179.096661  \n",
       "  24980  179.103661  \n",
       "  24981  179.111661  \n",
       "  24982  179.118661  \n",
       "  24983  179.125661  \n",
       "  24984  179.132661  \n",
       "  24985  179.139661  \n",
       "  24986  179.146661  \n",
       "  24987  179.154661  \n",
       "  24988  179.161661  \n",
       "  24989  179.168661  \n",
       "  24990  179.175661  \n",
       "  24991  179.182661  \n",
       "  24992  179.189661  \n",
       "  24993  179.197661  \n",
       "  24994  179.204661  \n",
       "  24995  179.211661  \n",
       "  24996  179.218661  \n",
       "  24997  179.225661  \n",
       "  24998  179.232661  \n",
       "  \n",
       "  [24999 rows x 9 columns],\n",
       "  'normal_2':        Voltage_01  Voltage_02  Acc_x  Acc_y  Acc_z  Gyro_x  Gyro_y  Gyro_z  \\\n",
       "  0        0.283105    0.031088   1.15  -4.44  14.82    3.19   -1.20   -0.42   \n",
       "  1        0.237443    0.031088   1.44  -4.85  14.87    3.27   -1.22   -0.46   \n",
       "  2        0.205479    0.036269   1.23  -5.08  15.49    3.31   -1.24   -0.49   \n",
       "  3        0.178082    0.036269   0.94  -5.40  15.46    3.35   -1.31   -0.52   \n",
       "  4        0.168950    0.036269   0.88  -6.03  15.70    3.38   -1.35   -0.54   \n",
       "  5        0.178082    0.031088   0.97  -6.16  16.05    3.40   -1.28   -0.57   \n",
       "  6        0.141553    0.036269   1.03  -5.93  15.73    3.40   -1.20   -0.60   \n",
       "  7        0.109589    0.041451   0.94  -5.39  15.28    3.40   -1.18   -0.63   \n",
       "  8        0.123288    0.041451   0.79  -5.74  15.58    3.38   -1.19   -0.66   \n",
       "  9        0.082192    0.041451   1.33  -5.49  15.31    3.34   -1.18   -0.68   \n",
       "  10       0.086758    0.046632   1.48  -5.68  15.07    3.28   -1.18   -0.71   \n",
       "  11       0.077626    0.041451   1.38  -5.36  15.39    3.17   -1.19   -0.73   \n",
       "  12       0.077626    0.041451   1.29  -5.21  14.81    3.05   -1.26   -0.75   \n",
       "  13       0.063927    0.041451   1.19  -4.81  14.29    2.97   -1.36   -0.77   \n",
       "  14       0.050228    0.041451   1.53  -4.17  14.07    2.96   -1.50   -0.80   \n",
       "  15       0.045662    0.041451   2.24  -3.87  14.11    3.09   -1.73   -0.90   \n",
       "  16       0.045662    0.041451   2.62  -1.48  12.71    3.19   -1.85   -0.91   \n",
       "  17       0.041096    0.041451   2.52  -0.04   8.94    2.97   -1.83   -0.92   \n",
       "  18       0.041096    0.046632   2.50   1.37   6.15    2.57   -1.74   -0.90   \n",
       "  19       0.036530    0.046632   3.02   5.75   3.79    2.17   -1.60   -0.88   \n",
       "  20       0.041096    0.041451   3.82  10.48   4.38    1.62   -1.41   -0.89   \n",
       "  21       0.041096    0.041451   5.77  11.83   6.74    1.10   -1.05   -0.86   \n",
       "  22       0.041096    0.046632   5.51   5.91  12.15    0.69   -0.43   -0.80   \n",
       "  23       0.045662    0.046632   5.60   4.48  17.44   -0.08    0.06   -0.73   \n",
       "  24       0.054795    0.041451   4.02   6.61  13.56   -0.63    0.23   -0.63   \n",
       "  25       0.045662    0.041451   3.21   7.59   9.55   -0.86    0.23   -0.55   \n",
       "  26       0.045662    0.041451   3.11   6.74   7.39   -1.16    0.15   -0.46   \n",
       "  27       0.045662    0.041451   2.68   6.42   6.66   -1.43    0.04   -0.39   \n",
       "  28       0.050228    0.041451   2.59   5.28   6.49   -1.54   -0.01   -0.30   \n",
       "  29       0.050228    0.041451   2.70   4.51   6.60   -1.57   -0.03   -0.24   \n",
       "  ...           ...         ...    ...    ...    ...     ...     ...     ...   \n",
       "  24970    0.080717    0.044199   6.08   2.39  31.38    4.28   -0.77   -1.37   \n",
       "  24971    0.071749    0.044199   8.02   5.08  31.54    4.28   -0.93   -1.44   \n",
       "  24972    0.058296    0.044199   8.90  10.94  28.22    4.28   -0.77   -1.43   \n",
       "  24973    0.058296    0.044199  11.63  18.82  21.45    4.28   -0.37   -1.59   \n",
       "  24974    0.049327    0.044199   7.17  18.86   7.63    4.23    0.01   -1.99   \n",
       "  24975    0.049327    0.044199   8.37  23.49  -6.77    3.23   -0.31   -2.48   \n",
       "  24976    0.053812    0.044199   6.87  20.44  18.37    1.63   -0.29   -2.66   \n",
       "  24977    0.067265    0.044199   8.95  14.50  30.38   -0.24    0.46   -2.44   \n",
       "  24978    0.053812    0.044199   7.03  12.55  24.01   -1.83    0.90   -2.14   \n",
       "  24979    0.049327    0.044199   4.10  12.53  16.24   -2.68    0.93   -1.86   \n",
       "  24980    0.044843    0.044199   2.86  11.75  11.81   -3.20    0.65   -1.51   \n",
       "  24981    0.044843    0.044199   3.29  10.46   9.67   -3.70    0.39   -1.25   \n",
       "  24982    0.044843    0.049724   3.71   8.83   8.83   -4.05    0.22   -1.04   \n",
       "  24983    0.049327    0.044199   4.28   7.25   7.75   -4.15    0.19   -0.91   \n",
       "  24984    0.049327    0.044199   4.60   5.75   7.07   -4.16    0.30   -0.77   \n",
       "  24985    0.040359    0.044199   4.57   4.68   6.58   -4.15    0.50   -0.64   \n",
       "  24986    0.040359    0.049724   4.54   3.94   6.07   -4.11    0.74   -0.53   \n",
       "  24987    0.040359    0.049724   4.53   3.45   5.90   -4.06    0.98   -0.43   \n",
       "  24988    0.040359    0.049724   4.26   3.13   6.03   -4.01    1.19   -0.36   \n",
       "  24989    0.040359    0.049724   3.90   2.88   6.55   -3.98    1.38   -0.29   \n",
       "  24990    0.044843    0.049724   3.55   2.76   7.38   -3.99    1.53   -0.22   \n",
       "  24991    0.040359    0.049724   3.22   2.64   8.21   -4.01    1.60   -0.14   \n",
       "  24992    0.040359    0.044199   2.89   2.59   9.08   -4.07    1.69   -0.06   \n",
       "  24993    0.035874    0.049724   2.53   2.43   9.71   -4.12    1.76    0.03   \n",
       "  24994    0.040359    0.044199   2.13   2.08  10.44   -4.19    1.81    0.15   \n",
       "  24995    0.040359    0.044199   1.72   1.62  11.35   -4.25    1.83    0.26   \n",
       "  24996    0.040359    0.049724   1.42   1.14  12.08   -4.28    1.83    0.36   \n",
       "  24997    0.040359    0.049724   1.28   0.67  12.76   -4.28    1.81    0.46   \n",
       "  24998    0.040359    0.049724   1.35   0.23  13.34   -4.28    1.80    0.53   \n",
       "  24999    0.040359    0.049724   1.34  -0.12  14.09   -4.28    1.81    0.58   \n",
       "  \n",
       "               Time  \n",
       "  0        0.017000  \n",
       "  1        0.023000  \n",
       "  2        0.029000  \n",
       "  3        0.035000  \n",
       "  4        0.041000  \n",
       "  5        0.048000  \n",
       "  6        0.055000  \n",
       "  7        0.061000  \n",
       "  8        0.067000  \n",
       "  9        0.073000  \n",
       "  10       0.079000  \n",
       "  11       0.086000  \n",
       "  12       0.092000  \n",
       "  13       0.098000  \n",
       "  14       0.104000  \n",
       "  15       0.110000  \n",
       "  16       0.117000  \n",
       "  17       0.123000  \n",
       "  18       0.130000  \n",
       "  19       0.136000  \n",
       "  20       0.142000  \n",
       "  21       0.148000  \n",
       "  22       0.154000  \n",
       "  23       0.160000  \n",
       "  24       0.166000  \n",
       "  25       0.173000  \n",
       "  26       0.179000  \n",
       "  27       0.185000  \n",
       "  28       0.192000  \n",
       "  29       0.198000  \n",
       "  ...           ...  \n",
       "  24970  159.255494  \n",
       "  24971  159.261494  \n",
       "  24972  159.267494  \n",
       "  24973  159.273494  \n",
       "  24974  159.280494  \n",
       "  24975  159.286494  \n",
       "  24976  159.292494  \n",
       "  24977  159.299494  \n",
       "  24978  159.306494  \n",
       "  24979  159.312494  \n",
       "  24980  159.318494  \n",
       "  24981  159.324494  \n",
       "  24982  159.331494  \n",
       "  24983  159.338494  \n",
       "  24984  159.344494  \n",
       "  24985  159.350494  \n",
       "  24986  159.356494  \n",
       "  24987  159.362494  \n",
       "  24988  159.369494  \n",
       "  24989  159.375494  \n",
       "  24990  159.382494  \n",
       "  24991  159.388494  \n",
       "  24992  159.394494  \n",
       "  24993  159.400494  \n",
       "  24994  159.407494  \n",
       "  24995  159.413494  \n",
       "  24996  159.419494  \n",
       "  24997  159.426494  \n",
       "  24998  159.432494  \n",
       "  24999  159.439494  \n",
       "  \n",
       "  [25000 rows x 9 columns],\n",
       "  'normal_3':        Voltage_01  Voltage_02  Acc_x  Acc_y  Acc_z  Gyro_x  Gyro_y  Gyro_z  \\\n",
       "  0        0.038674    0.048387   2.16  -2.85  15.04   -4.27    1.48    0.57   \n",
       "  1        0.044199    0.048387   1.84  -3.57  15.73   -4.26    1.59    0.55   \n",
       "  2        0.044199    0.048387   1.47  -4.42  16.47   -4.25    1.69    0.51   \n",
       "  3        0.044199    0.048387   1.19  -5.12  17.10   -4.24    1.76    0.47   \n",
       "  4        0.049724    0.048387   0.82  -5.72  17.77   -4.22    1.79    0.44   \n",
       "  5        0.044199    0.048387   0.53  -6.41  18.16   -4.20    1.78    0.42   \n",
       "  6        0.044199    0.048387   0.33  -6.93  18.19   -4.17    1.73    0.42   \n",
       "  7        0.044199    0.048387   0.25  -7.34  18.27   -4.13    1.65    0.45   \n",
       "  8        0.038674    0.048387   0.20  -7.78  18.23   -4.04    1.57    0.51   \n",
       "  9        0.049724    0.048387   0.10  -8.07  18.18   -3.91    1.47    0.58   \n",
       "  10       0.044199    0.048387   0.06  -8.23  18.08   -3.76    1.38    0.67   \n",
       "  11       0.044199    0.048387   0.01  -8.37  18.02   -3.60    1.30    0.76   \n",
       "  12       0.044199    0.053763  -0.09  -8.51  18.40   -3.46    1.22    0.84   \n",
       "  13       0.049724    0.048387  -0.37  -8.61  18.72   -3.29    1.14    0.91   \n",
       "  14       0.044199    0.048387  -0.80  -8.57  19.05   -3.16    1.06    0.95   \n",
       "  15       0.038674    0.048387  -1.01  -8.59  19.31   -3.05    0.97    0.99   \n",
       "  16       0.044199    0.048387  -1.15  -8.69  19.49   -2.99    0.85    1.00   \n",
       "  17       0.044199    0.048387  -1.14  -9.01  19.66   -2.94    0.72    0.98   \n",
       "  18       0.044199    0.048387  -1.00  -9.41  19.72   -2.90    0.61    0.94   \n",
       "  19       0.038674    0.048387  -0.76  -9.94  19.64   -2.87    0.55    0.89   \n",
       "  20       0.044199    0.053763  -0.40 -11.01  19.61   -2.82    0.55    0.83   \n",
       "  21       0.044199    0.048387  -0.11 -10.92  19.60   -2.79    0.60    0.77   \n",
       "  22       0.038674    0.048387  -0.19 -11.46  19.75   -2.78    0.67    0.69   \n",
       "  23       0.049724    0.048387  -0.39 -11.91  19.62   -2.81    0.76    0.61   \n",
       "  24       0.044199    0.048387  -0.47 -12.36  18.92   -2.89    0.86    0.53   \n",
       "  25       0.044199    0.048387  -0.53 -13.10  17.86   -2.99    0.95    0.46   \n",
       "  26       0.044199    0.048387  -0.81 -13.77  16.14   -3.05    1.06    0.40   \n",
       "  27       0.055249    0.048387  -1.34 -14.59  13.85   -3.06    1.27    0.43   \n",
       "  28       0.044199    0.048387  -2.14 -15.73  11.58   -2.96    1.59    0.55   \n",
       "  29       0.044199    0.048387  -3.32 -16.89  10.37   -2.75    1.89    0.74   \n",
       "  ...           ...         ...    ...    ...    ...     ...     ...     ...   \n",
       "  29970    0.038835    0.063380 -11.74 -33.57  66.22    2.07   -0.39   -0.44   \n",
       "  29971    0.043689    0.056338  -5.32 -18.32   1.53    3.13   -0.42   -0.51   \n",
       "  29972    0.043689    0.056338  -4.50 -16.66   3.31    3.53   -0.97   -0.71   \n",
       "  29973    0.038835    0.056338  -2.89 -16.17   6.91    3.67   -1.54   -0.84   \n",
       "  29974    0.038835    0.070423   2.37  -6.10   9.69    3.59   -2.18   -0.97   \n",
       "  29975    0.033981    0.070423   2.86   2.51  11.85    3.36   -2.46   -0.96   \n",
       "  29976    0.038835    0.112676   7.07   4.60  14.94    3.25   -1.79   -0.54   \n",
       "  29977    0.029126    0.246479  11.08   4.33  12.40    3.23   -0.68    0.08   \n",
       "  29978    0.029126    0.345070   8.71  -3.51  13.56    3.50    0.12    0.53   \n",
       "  29979    0.029126    0.394366   0.31  -5.15  24.11    3.95    0.00    0.61   \n",
       "  29980    0.024272    0.535211  -5.85  -4.79  25.13    3.80   -0.69    0.32   \n",
       "  29981    0.019417    0.584507  -1.75  -3.87  21.69    3.48   -1.34   -0.02   \n",
       "  29982    0.014563    0.661972   2.86  -3.11  18.75    3.26   -1.22    0.05   \n",
       "  29983    0.014563    0.697183   0.81  -1.86  17.31    2.99   -0.86    0.18   \n",
       "  29984    0.019417    0.683099  -1.24  -1.90  16.91    2.69   -0.80    0.07   \n",
       "  29985    0.014563    0.661972  -0.47  -1.68  14.85    2.52   -0.80   -0.06   \n",
       "  29986    0.019417    0.640845   0.65  -2.53  21.91    2.63   -0.74   -0.11   \n",
       "  29987    0.024272    0.633803   1.01  -0.66  15.80    1.03   -1.39   -0.07   \n",
       "  29988    0.024272    0.669014   0.96  -1.77  11.75    0.14   -0.60    0.04   \n",
       "  29989    0.024272    0.647887   0.26  -1.89  10.51   -0.05   -0.18    0.06   \n",
       "  29990    0.024272    0.640845   0.44  -1.12  10.42   -0.06   -0.09    0.06   \n",
       "  29991    0.024272    0.661972   0.43  -0.81  10.16   -0.05   -0.07    0.04   \n",
       "  29992    0.019417    0.683099   0.60  -0.70   9.63   -0.05   -0.05    0.02   \n",
       "  29993    0.024272    0.718310   0.70  -0.98  10.05   -0.05   -0.05    0.02   \n",
       "  29994    0.024272    0.739437   0.60  -1.07  10.23   -0.05   -0.06    0.02   \n",
       "  29995    0.019417    0.732394   0.57  -0.83  10.08   -0.04   -0.07    0.02   \n",
       "  29996    0.024272    0.746479   0.53  -0.76  10.05   -0.03   -0.07    0.02   \n",
       "  29997    0.024272    0.781690   0.59  -0.77  10.02   -0.03   -0.06    0.02   \n",
       "  29998    0.024272    0.767606   0.60  -0.76  10.08   -0.03   -0.05    0.02   \n",
       "  29999    0.024272    0.767606   0.67  -0.66  10.05   -0.03   -0.06    0.03   \n",
       "  \n",
       "               Time  \n",
       "  0        0.023000  \n",
       "  1        0.029000  \n",
       "  2        0.035000  \n",
       "  3        0.041000  \n",
       "  4        0.048000  \n",
       "  5        0.054000  \n",
       "  6        0.060000  \n",
       "  7        0.066000  \n",
       "  8        0.072000  \n",
       "  9        0.078000  \n",
       "  10       0.084000  \n",
       "  11       0.091000  \n",
       "  12       0.097000  \n",
       "  13       0.103000  \n",
       "  14       0.110000  \n",
       "  15       0.116000  \n",
       "  16       0.122000  \n",
       "  17       0.129000  \n",
       "  18       0.135000  \n",
       "  19       0.141000  \n",
       "  20       0.147000  \n",
       "  21       0.153000  \n",
       "  22       0.160000  \n",
       "  23       0.166000  \n",
       "  24       0.173000  \n",
       "  25       0.179000  \n",
       "  26       0.185000  \n",
       "  27       0.191000  \n",
       "  28       0.198000  \n",
       "  29       0.204000  \n",
       "  ...           ...  \n",
       "  29970  191.023853  \n",
       "  29971  191.031853  \n",
       "  29972  191.037853  \n",
       "  29973  191.043853  \n",
       "  29974  191.049853  \n",
       "  29975  191.056853  \n",
       "  29976  191.062853  \n",
       "  29977  191.068853  \n",
       "  29978  191.075853  \n",
       "  29979  191.082853  \n",
       "  29980  191.088853  \n",
       "  29981  191.094853  \n",
       "  29982  191.101853  \n",
       "  29983  191.107853  \n",
       "  29984  191.114853  \n",
       "  29985  191.120853  \n",
       "  29986  191.127853  \n",
       "  29987  191.133853  \n",
       "  29988  191.139853  \n",
       "  29989  191.145853  \n",
       "  29990  191.152853  \n",
       "  29991  191.159853  \n",
       "  29992  191.165853  \n",
       "  29993  191.171853  \n",
       "  29994  191.177853  \n",
       "  29995  191.184853  \n",
       "  29996  191.190853  \n",
       "  29997  191.196853  \n",
       "  29998  191.203853  \n",
       "  29999  191.210853  \n",
       "  \n",
       "  [30000 rows x 9 columns],\n",
       "  'ran':        Voltage_01  Voltage_02  Acc_x  Acc_y  Acc_z  Gyro_x  Gyro_y  Gyro_z  \\\n",
       "  0        0.076271    0.083333   0.03 -13.06   0.59    0.86   -1.03   -0.99   \n",
       "  1        0.067797    0.089744   1.91 -12.01   3.41    1.45   -1.00   -1.06   \n",
       "  2        0.067797    0.108974   0.29  -5.43   8.20    1.94   -0.84   -1.02   \n",
       "  3        0.067797    0.128205   0.94   0.84  12.09    2.02   -0.67   -0.90   \n",
       "  4        0.067797    0.128205   2.15   2.11  13.40    1.80   -0.48   -0.69   \n",
       "  5        0.067797    0.192308   3.01   1.05  12.58    1.37   -0.31   -0.49   \n",
       "  6        0.067797    0.211538   2.52  -0.54  11.04    1.16   -0.14   -0.26   \n",
       "  7        0.059322    0.243590   1.93  -1.05   9.75    1.22   -0.05   -0.08   \n",
       "  8        0.042373    0.294872   1.08  -1.38  10.05    1.47   -0.01    0.06   \n",
       "  9        0.050847    0.307692   0.52  -1.41  11.42    1.57    0.01    0.13   \n",
       "  10       0.050847    0.326923  -0.27  -1.67  12.41    1.52    0.04    0.15   \n",
       "  11       0.059322    0.339744  -0.90  -2.34  12.55    1.43    0.10    0.11   \n",
       "  12       0.050847    0.378205  -1.17  -2.25  12.00    1.36    0.16    0.04   \n",
       "  13       0.050847    0.384615  -0.74  -2.06  11.75    1.32    0.18   -0.07   \n",
       "  14       0.033898    0.410256   0.10  -2.16  12.10    1.24    0.18   -0.13   \n",
       "  15       0.042373    0.455128   0.30  -2.20  11.87    1.10    0.22   -0.13   \n",
       "  16       0.050847    0.442308   0.57  -2.28  11.73    0.98    0.25   -0.09   \n",
       "  17       0.050847    0.435897   0.22  -2.91  13.94    0.44    0.07   -0.04   \n",
       "  18       0.050847    0.474359   0.32  -2.39  10.69   -0.04    0.00   -0.01   \n",
       "  19       0.050847    0.519231  -0.24  -1.88   9.95   -0.07    0.03   -0.04   \n",
       "  20       0.042373    0.474359   0.29  -1.42   9.74   -0.06    0.06   -0.04   \n",
       "  21       0.050847    0.551282   0.46  -1.55   9.73   -0.09    0.06   -0.04   \n",
       "  22       0.050847    0.532051   0.36  -1.60   9.83   -0.10    0.05   -0.04   \n",
       "  23       0.059322    0.525641   0.18  -1.31   9.56   -0.12    0.05   -0.03   \n",
       "  24       0.042373    0.570513   0.23  -1.34   9.54   -0.14    0.05   -0.03   \n",
       "  25       0.042373    0.564103   0.17  -1.39   9.73   -0.16    0.06   -0.03   \n",
       "  26       0.042373    0.628205   0.20  -1.53   9.82   -0.17    0.06   -0.03   \n",
       "  27       0.059322    0.621795   0.15  -1.44   9.76   -0.19    0.06   -0.03   \n",
       "  28       0.067797    0.660256   0.16  -1.37   9.66   -0.19    0.07   -0.03   \n",
       "  29       0.050847    0.711538   0.32  -1.49   9.69   -0.20    0.07   -0.03   \n",
       "  ...           ...         ...    ...    ...    ...     ...     ...     ...   \n",
       "  14883    0.033333    0.550265  -0.80  -3.48  15.62    2.22   -0.48   -0.11   \n",
       "  14884    0.033333    0.608466   0.42  -2.58  13.41    2.03   -0.46   -0.13   \n",
       "  14885    0.033333    0.650794   1.50  -2.38  13.59    1.95   -0.26    0.04   \n",
       "  14886    0.033333    0.677249   1.17  -2.35  12.59    1.83    0.02    0.25   \n",
       "  14887    0.025000    0.708995   0.13  -2.49  13.84    1.61    0.18    0.40   \n",
       "  14888    0.025000    0.719577  -2.57  -3.02  13.89    0.52   -0.39    0.23   \n",
       "  14889    0.016667    0.724868  -1.26  -2.35  11.07   -0.11   -0.18    0.15   \n",
       "  14890    0.025000    0.761905  -0.73  -2.30   9.99   -0.11   -0.09    0.05   \n",
       "  14891    0.025000    0.730159   0.53  -1.37   9.84   -0.11   -0.04    0.04   \n",
       "  14892    0.033333    0.730159   0.39  -1.34   9.86   -0.10   -0.05    0.06   \n",
       "  14893    0.050000    0.730159   0.02  -1.37   9.64   -0.09   -0.06    0.05   \n",
       "  14894    0.041667    0.756614  -0.01  -1.45   9.67   -0.11   -0.05    0.04   \n",
       "  14895    0.041667    0.735450   0.14  -1.44   9.73   -0.11   -0.04    0.01   \n",
       "  14896    0.041667    0.751323   0.06  -1.40   9.70   -0.11   -0.04   -0.01   \n",
       "  14897    0.050000    0.772487   0.01  -1.38   9.76   -0.11   -0.03   -0.02   \n",
       "  14898    0.041667    0.783069  -0.57  -1.39   9.77   -0.11   -0.02   -0.03   \n",
       "  14899    0.041667    0.751323   0.04  -1.30   9.77   -0.10   -0.00   -0.04   \n",
       "  14900    0.050000    0.767196   0.06  -1.20   9.72   -0.10    0.01   -0.04   \n",
       "  14901    0.041667    0.777778   0.03  -0.63   9.72   -0.10    0.02   -0.04   \n",
       "  14902    0.041667    0.804233   0.04  -1.24   9.79   -0.10    0.03   -0.04   \n",
       "  14903    0.041667    0.788360   0.04  -1.20   9.80   -0.09    0.04   -0.03   \n",
       "  14904    0.050000    0.793651   0.07  -1.19   9.79   -0.07    0.04   -0.03   \n",
       "  14905    0.050000    0.761905   0.05  -1.15   9.80   -0.06    0.05   -0.03   \n",
       "  14906    0.050000    0.788360   0.07  -1.18   9.82   -0.04    0.05   -0.03   \n",
       "  14907    0.050000    0.767196   0.07  -1.15   9.81   -0.03    0.05   -0.02   \n",
       "  14908    0.041667    0.756614   0.05  -1.14   9.78   -0.02    0.06   -0.02   \n",
       "  14909    0.058333    0.746032   0.05  -1.17   9.75   -0.01    0.06   -0.02   \n",
       "  14910    0.058333    0.735450   0.12  -1.12   9.73   -0.00    0.06   -0.02   \n",
       "  14911    0.050000    0.730159   0.10  -1.17   9.74    0.01    0.05   -0.02   \n",
       "  14912    0.058333    0.730159   0.11  -1.14   9.75    0.02    0.05   -0.02   \n",
       "  \n",
       "               Time  \n",
       "  0        0.017000  \n",
       "  1        0.024000  \n",
       "  2        0.031000  \n",
       "  3        0.038000  \n",
       "  4        0.046000  \n",
       "  5        0.054000  \n",
       "  6        0.061000  \n",
       "  7        0.067000  \n",
       "  8        0.074000  \n",
       "  9        0.082000  \n",
       "  10       0.090000  \n",
       "  11       0.097000  \n",
       "  12       0.104000  \n",
       "  13       0.111000  \n",
       "  14       0.118000  \n",
       "  15       0.125000  \n",
       "  16       0.132000  \n",
       "  17       0.139000  \n",
       "  18       0.146000  \n",
       "  19       0.153000  \n",
       "  20       0.160000  \n",
       "  21       0.167000  \n",
       "  22       0.175000  \n",
       "  23       0.182000  \n",
       "  24       0.189000  \n",
       "  25       0.196000  \n",
       "  26       0.203000  \n",
       "  27       0.210000  \n",
       "  28       0.218000  \n",
       "  29       0.225000  \n",
       "  ...           ...  \n",
       "  14883  107.371463  \n",
       "  14884  107.378463  \n",
       "  14885  107.385463  \n",
       "  14886  107.393463  \n",
       "  14887  107.400463  \n",
       "  14888  107.407463  \n",
       "  14889  107.414463  \n",
       "  14890  107.420463  \n",
       "  14891  107.427463  \n",
       "  14892  107.435463  \n",
       "  14893  107.442463  \n",
       "  14894  107.449463  \n",
       "  14895  107.456463  \n",
       "  14896  107.463463  \n",
       "  14897  107.470463  \n",
       "  14898  107.478463  \n",
       "  14899  107.485463  \n",
       "  14900  107.492463  \n",
       "  14901  107.499463  \n",
       "  14902  107.506463  \n",
       "  14903  107.513463  \n",
       "  14904  107.521463  \n",
       "  14905  107.528463  \n",
       "  14906  107.535463  \n",
       "  14907  107.542463  \n",
       "  14908  107.549463  \n",
       "  14909  107.556463  \n",
       "  14910  107.564463  \n",
       "  14911  107.571463  \n",
       "  14912  107.578463  \n",
       "  \n",
       "  [14913 rows x 9 columns]},\n",
       " 'Side Walk': {'ran_side':        Voltage_01  Voltage_02  Acc_x  Acc_y  Acc_z  Gyro_x  Gyro_y  Gyro_z  \\\n",
       "  0        0.113636    0.094595  -3.35  -0.67   9.66   -0.50   -0.30    0.28   \n",
       "  1        0.102273    0.155405  -2.60  -0.94   9.94   -0.37   -0.16   -0.22   \n",
       "  2        0.102273    0.148649   1.92  -0.65  11.49   -0.10    0.05   -0.13   \n",
       "  3        0.102273    0.168919   1.28  -0.80  10.73   -0.16    0.01    0.03   \n",
       "  4        0.159091    0.175676  -0.59  -1.06  10.24   -0.07   -0.10    0.00   \n",
       "  5        0.159091    0.175676  -0.35  -1.13   9.62   -0.05   -0.02   -0.03   \n",
       "  6        0.193182    0.162162   0.03  -1.14   9.64   -0.05    0.07   -0.02   \n",
       "  7        0.193182    0.168919   0.03  -0.93   9.52   -0.05    0.11   -0.02   \n",
       "  8        0.193182    0.182432  -0.07  -0.95   9.56   -0.04    0.15   -0.02   \n",
       "  9        0.193182    0.175676   0.03  -1.25   9.74   -0.04    0.18   -0.01   \n",
       "  10       0.204545    0.168919   0.21  -1.37   9.73   -0.07    0.21   -0.01   \n",
       "  11       0.193182    0.189189   0.35  -1.28   9.96   -0.10    0.20   -0.01   \n",
       "  12       0.215909    0.182432   0.41  -1.19   9.92   -0.10    0.18   -0.02   \n",
       "  13       0.204545    0.195946   0.45  -1.15   9.87   -0.11    0.16   -0.02   \n",
       "  14       0.227273    0.182432   0.38  -1.09   9.79   -0.11    0.14   -0.02   \n",
       "  15       0.227273    0.216216   0.30  -1.07   9.72   -0.10    0.13   -0.03   \n",
       "  16       0.215909    0.202703   0.23  -1.00   9.69   -0.11    0.13   -0.03   \n",
       "  17       0.204545    0.222973   0.16  -0.97   9.78   -0.11    0.14   -0.03   \n",
       "  18       0.215909    0.222973   0.18  -0.97   9.81   -0.11    0.14   -0.04   \n",
       "  19       0.215909    0.250000   0.20  -0.91   9.80   -0.10    0.15   -0.03   \n",
       "  20       0.215909    0.263514   0.22  -0.88   9.83   -0.08    0.15   -0.03   \n",
       "  21       0.227273    0.263514   0.23  -0.89   9.81   -0.06    0.15   -0.03   \n",
       "  22       0.227273    0.263514   0.29  -0.88   9.82   -0.04    0.14   -0.02   \n",
       "  23       0.227273    0.256757   0.28  -0.89   9.82   -0.03    0.14   -0.02   \n",
       "  24       0.227273    0.243243   0.30  -0.89   9.82   -0.01    0.12   -0.02   \n",
       "  25       0.215909    0.250000   0.33  -0.92   9.82    0.00    0.11   -0.01   \n",
       "  26       0.227273    0.236486   0.36  -0.92   9.77    0.02    0.10   -0.02   \n",
       "  27       0.238636    0.216216   0.35  -0.93   9.77    0.02    0.08   -0.02   \n",
       "  28       0.238636    0.216216   0.38  -0.91   9.74    0.02    0.06   -0.01   \n",
       "  29       0.238636    0.202703   0.37  -0.93   9.77    0.02    0.05   -0.02   \n",
       "  ...           ...         ...    ...    ...    ...     ...     ...     ...   \n",
       "  24970    0.130435    0.402778  -0.32  -0.45   8.44    0.20   -0.56    0.14   \n",
       "  24971    0.130435    0.361111  -1.04  -0.88   7.38    0.11   -0.45    0.07   \n",
       "  24972    0.173913    0.326389  -0.58  -1.88   9.06    0.02   -0.26   -0.00   \n",
       "  24973    0.217391    0.326389  -0.24  -1.83  10.44   -0.06   -0.14   -0.02   \n",
       "  24974    0.217391    0.333333  -0.31  -0.70  10.29   -0.02   -0.08   -0.02   \n",
       "  24975    0.246377    0.326389  -0.35  -0.80  10.03    0.04   -0.02    0.00   \n",
       "  24976    0.318841    0.277778   0.12  -1.10  10.03    0.06    0.01    0.02   \n",
       "  24977    0.333333    0.284722   0.33  -1.18   9.93    0.05   -0.01    0.02   \n",
       "  24978    0.391304    0.277778   0.24  -1.08   9.75    0.03   -0.03    0.02   \n",
       "  24979    0.376812    0.263889   0.11  -0.83   9.75    0.01   -0.03    0.02   \n",
       "  24980    0.391304    0.229167   0.05  -0.83   9.82   -0.00   -0.02    0.02   \n",
       "  24981    0.391304    0.250000   0.10  -0.78   9.82    0.01   -0.03    0.02   \n",
       "  24982    0.420290    0.222222  -0.03  -0.82   9.65    0.02   -0.03    0.03   \n",
       "  24983    0.391304    0.243056   0.03  -0.76   9.82    0.02   -0.03    0.03   \n",
       "  24984    0.391304    0.263889   0.03  -0.72   9.98    0.03   -0.03    0.03   \n",
       "  24985    0.405797    0.263889  -0.05  -0.73   9.93    0.05   -0.04    0.03   \n",
       "  24986    0.391304    0.222222  -0.06  -0.75   9.84    0.08   -0.03    0.04   \n",
       "  24987    0.420290    0.208333   0.01  -0.82   9.80    0.08   -0.02    0.03   \n",
       "  24988    0.420290    0.215278   0.02  -0.83   9.79    0.08   -0.02    0.04   \n",
       "  24989    0.420290    0.194444   0.04  -0.82   9.70    0.08   -0.02    0.03   \n",
       "  24990    0.434783    0.180556   0.02  -0.85   9.65    0.07   -0.01    0.03   \n",
       "  24991    0.434783    0.166667   0.02  -0.85   9.65    0.06   -0.00    0.03   \n",
       "  24992    0.420290    0.180556   0.01  -0.86   9.68    0.05   -0.00    0.03   \n",
       "  24993    0.420290    0.166667   0.05  -0.88   9.64    0.04    0.01    0.02   \n",
       "  24994    0.420290    0.173611   0.05  -0.91   9.60    0.02    0.01    0.03   \n",
       "  24995    0.434783    0.173611   0.15  -0.90   9.66    0.02    0.01    0.02   \n",
       "  24996    0.405797    0.173611   0.14  -0.87   9.67    0.01    0.01    0.02   \n",
       "  24997    0.376812    0.159722   0.13  -0.88   9.66   -0.00    0.01    0.01   \n",
       "  24998    0.362319    0.166667   0.05  -0.90   9.68   -0.00    0.01    0.01   \n",
       "  24999    0.376812    0.145833   0.10  -0.89   9.72   -0.01    0.01    0.01   \n",
       "  \n",
       "               Time  \n",
       "  0        0.024000  \n",
       "  1        0.031000  \n",
       "  2        0.038000  \n",
       "  3        0.046000  \n",
       "  4        0.053000  \n",
       "  5        0.060000  \n",
       "  6        0.067000  \n",
       "  7        0.074000  \n",
       "  8        0.081000  \n",
       "  9        0.089000  \n",
       "  10       0.096000  \n",
       "  11       0.103000  \n",
       "  12       0.110000  \n",
       "  13       0.117000  \n",
       "  14       0.124000  \n",
       "  15       0.132000  \n",
       "  16       0.139000  \n",
       "  17       0.146000  \n",
       "  18       0.153000  \n",
       "  19       0.160000  \n",
       "  20       0.167000  \n",
       "  21       0.175000  \n",
       "  22       0.182000  \n",
       "  23       0.189000  \n",
       "  24       0.196000  \n",
       "  25       0.203000  \n",
       "  26       0.210000  \n",
       "  27       0.218000  \n",
       "  28       0.225000  \n",
       "  29       0.232000  \n",
       "  ...           ...  \n",
       "  24970  178.980656  \n",
       "  24971  178.988656  \n",
       "  24972  178.995656  \n",
       "  24973  179.003656  \n",
       "  24974  179.009656  \n",
       "  24975  179.016656  \n",
       "  24976  179.023656  \n",
       "  24977  179.031656  \n",
       "  24978  179.038656  \n",
       "  24979  179.045656  \n",
       "  24980  179.052656  \n",
       "  24981  179.059656  \n",
       "  24982  179.066656  \n",
       "  24983  179.074656  \n",
       "  24984  179.081656  \n",
       "  24985  179.088656  \n",
       "  24986  179.095656  \n",
       "  24987  179.102656  \n",
       "  24988  179.109656  \n",
       "  24989  179.117656  \n",
       "  24990  179.124656  \n",
       "  24991  179.131656  \n",
       "  24992  179.138656  \n",
       "  24993  179.145656  \n",
       "  24994  179.152656  \n",
       "  24995  179.160656  \n",
       "  24996  179.167656  \n",
       "  24997  179.174656  \n",
       "  24998  179.181656  \n",
       "  24999  179.188656  \n",
       "  \n",
       "  [25000 rows x 9 columns],\n",
       "  'side':        Voltage_01  Voltage_02  Acc_x  Acc_y  Acc_z  Gyro_x  Gyro_y  Gyro_z  \\\n",
       "  0        0.042105    0.396552   0.30  -0.74  11.29   -0.05    0.22   -0.09   \n",
       "  1        0.042105    0.413793   1.98  -0.68  10.43    0.00    0.02   -0.07   \n",
       "  2        0.042105    0.413793   3.39  -0.82  10.48    0.02   -0.08    0.07   \n",
       "  3        0.052632    0.370690   2.31  -0.59  10.25    0.06   -0.11    0.19   \n",
       "  4        0.052632    0.353448   2.21  -0.23  10.21    0.05   -0.20    0.29   \n",
       "  5        0.042105    0.336207   0.68  -0.55   9.92    0.01   -0.22    0.34   \n",
       "  6        0.063158    0.344828  -0.19  -0.91   9.19    0.00   -0.18    0.27   \n",
       "  7        0.063158    0.362069  -0.64  -0.95   9.74    0.00   -0.06    0.26   \n",
       "  8        0.073684    0.344828  -0.33  -0.50   9.58    0.00    0.06    0.25   \n",
       "  9        0.052632    0.387931   0.14  -0.84   9.79   -0.02    0.12    0.24   \n",
       "  10       0.052632    0.370690   0.54  -1.97  10.36   -0.05    0.13    0.23   \n",
       "  11       0.052632    0.448276   1.32  -0.78   9.80   -0.08    0.10    0.22   \n",
       "  12       0.063158    0.456897   1.11  -1.37  10.18   -0.10    0.05    0.20   \n",
       "  13       0.063158    0.500000   1.05  -0.93  10.29   -0.10    0.03    0.17   \n",
       "  14       0.052632    0.543103   0.72  -0.78  10.04   -0.09    0.01    0.14   \n",
       "  15       0.042105    0.629310   0.77  -0.74  10.08   -0.08    0.00    0.12   \n",
       "  16       0.063158    0.663793   0.85  -0.75  10.02   -0.07   -0.01    0.12   \n",
       "  17       0.042105    0.732759   0.81  -0.76  10.11   -0.06   -0.01    0.12   \n",
       "  18       0.031579    0.715517   0.80  -0.76  10.13   -0.06   -0.02    0.12   \n",
       "  19       0.031579    0.767241   0.73  -0.74  10.09   -0.05   -0.02    0.11   \n",
       "  20       0.052632    0.810345   0.74  -0.78   9.98   -0.05   -0.02    0.11   \n",
       "  21       0.052632    0.801724   0.75  -0.76   9.97   -0.04   -0.03    0.11   \n",
       "  22       0.063158    0.827586   0.72  -0.74   9.97   -0.04   -0.03    0.11   \n",
       "  23       0.042105    0.887931   1.07  -0.74  10.58   -0.04   -0.03    0.10   \n",
       "  24       0.052632    0.879310   0.71  -0.72   9.97   -0.04   -0.00    0.10   \n",
       "  25       0.052632    0.896552   0.69  -0.69   9.90   -0.04   -0.03    0.09   \n",
       "  26       0.042105    0.896552   0.69  -0.70   9.89   -0.03   -0.03    0.09   \n",
       "  27       0.042105    0.931034   0.78  -0.73   9.92   -0.03   -0.02    0.09   \n",
       "  28       0.042105    0.913793   0.77  -0.71  10.05   -0.03   -0.03    0.09   \n",
       "  29       0.042105    0.939655   0.74  -0.73  10.04   -0.02   -0.02    0.08   \n",
       "  ...           ...         ...    ...    ...    ...     ...     ...     ...   \n",
       "  24970    0.563492    0.038462   4.88  -3.07  11.51    0.83   -1.77   -0.62   \n",
       "  24971    0.563492    0.023077   1.55  -2.11  11.71    0.94   -1.59   -0.87   \n",
       "  24972    0.523810    0.023077   1.78  -1.68  14.13    1.18   -1.68   -0.98   \n",
       "  24973    0.500000    0.023077   1.65  -2.10  13.60    1.30   -1.97   -1.04   \n",
       "  24974    0.468254    0.030769   1.71  -2.46  11.57    1.41   -1.92   -1.13   \n",
       "  24975    0.412698    0.030769   2.81  -3.04  11.19    1.48   -1.72   -1.28   \n",
       "  24976    0.309524    0.038462   2.74  -3.18  11.58    1.52   -1.46   -1.44   \n",
       "  24977    0.285714    0.038462   2.28  -2.92  11.75    1.54   -1.26   -1.58   \n",
       "  24978    0.190476    0.038462   3.36  -2.57   9.71    1.85   -1.06   -1.68   \n",
       "  24979    0.166667    0.038462   2.89  -2.80  12.05    1.53   -0.94   -1.80   \n",
       "  24980    0.119048    0.038462   2.68  -2.77  11.86    1.48   -0.84   -1.91   \n",
       "  24981    0.087302    0.046154   3.42  -2.73  11.65    1.42   -0.81   -2.03   \n",
       "  24982    0.087302    0.046154   4.00  -2.67  11.23    1.33   -0.76   -2.15   \n",
       "  24983    0.071429    0.046154   4.16  -2.26  11.18    1.22   -0.72   -2.25   \n",
       "  24984    0.071429    0.046154   3.82  -2.12  11.44    1.10   -0.69   -2.40   \n",
       "  24985    0.047619    0.046154   3.31  -1.68  12.03    0.96   -0.78   -2.53   \n",
       "  24986    0.039683    0.046154   2.16  -0.74  12.20    0.78   -1.03   -2.61   \n",
       "  24987    0.047619    0.046154   1.63   0.26  10.93    0.54   -1.30   -2.69   \n",
       "  24988    0.047619    0.046154   1.32   2.23   9.96    0.29   -1.55   -2.72   \n",
       "  24989    0.039683    0.046154   1.79   3.62   8.72    0.06   -1.70   -2.69   \n",
       "  24990    0.063492    0.046154   3.85   4.68   7.43   -0.14   -1.71   -2.55   \n",
       "  24991    0.039683    0.046154   5.84   3.55   7.43   -0.27   -1.50   -2.21   \n",
       "  24992    0.039683    0.046154   6.57   1.10   9.11   -0.37   -0.89   -1.75   \n",
       "  24993    0.047619    0.046154   7.44   0.17  11.35   -0.44   -0.12   -1.36   \n",
       "  24994    0.039683    0.046154   7.24   0.45  10.17   -0.56    0.34   -1.09   \n",
       "  24995    0.039683    0.046154   5.60   1.00   8.34   -0.64    0.50   -0.87   \n",
       "  24996    0.039683    0.046154   4.36   0.94   6.92   -0.65    0.58   -0.66   \n",
       "  24997    0.039683    0.046154   3.65   0.27   6.34   -0.58    0.54   -0.45   \n",
       "  24998    0.039683    0.046154   3.87  -0.63   6.48   -0.40    0.48   -0.29   \n",
       "  24999    0.031746    0.046154   4.55  -1.07   7.11   -0.19    0.49   -0.11   \n",
       "  \n",
       "               Time  \n",
       "  0        0.023000  \n",
       "  1        0.029000  \n",
       "  2        0.035000  \n",
       "  3        0.041000  \n",
       "  4        0.048000  \n",
       "  5        0.054000  \n",
       "  6        0.060000  \n",
       "  7        0.067000  \n",
       "  8        0.073000  \n",
       "  9        0.079000  \n",
       "  10       0.086000  \n",
       "  11       0.092000  \n",
       "  12       0.098000  \n",
       "  13       0.104000  \n",
       "  14       0.110000  \n",
       "  15       0.116000  \n",
       "  16       0.122000  \n",
       "  17       0.130000  \n",
       "  18       0.136000  \n",
       "  19       0.142000  \n",
       "  20       0.148000  \n",
       "  21       0.154000  \n",
       "  22       0.160000  \n",
       "  23       0.166000  \n",
       "  24       0.173000  \n",
       "  25       0.179000  \n",
       "  26       0.185000  \n",
       "  27       0.192000  \n",
       "  28       0.198000  \n",
       "  29       0.204000  \n",
       "  ...           ...  \n",
       "  24970  158.473364  \n",
       "  24971  158.479364  \n",
       "  24972  158.487364  \n",
       "  24973  158.493364  \n",
       "  24974  158.499364  \n",
       "  24975  158.505364  \n",
       "  24976  158.512364  \n",
       "  24977  158.518364  \n",
       "  24978  158.525364  \n",
       "  24979  158.531364  \n",
       "  24980  158.538364  \n",
       "  24981  158.544364  \n",
       "  24982  158.550364  \n",
       "  24983  158.556364  \n",
       "  24984  158.563364  \n",
       "  24985  158.570364  \n",
       "  24986  158.576364  \n",
       "  24987  158.582364  \n",
       "  24988  158.588364  \n",
       "  24989  158.595364  \n",
       "  24990  158.601364  \n",
       "  24991  158.607364  \n",
       "  24992  158.614364  \n",
       "  24993  158.620364  \n",
       "  24994  158.627364  \n",
       "  24995  158.633364  \n",
       "  24996  158.639364  \n",
       "  24997  158.645364  \n",
       "  24998  158.652364  \n",
       "  24999  158.658364  \n",
       "  \n",
       "  [25000 rows x 9 columns],\n",
       "  'side_2':        Voltage_01  Voltage_02  Acc_x  Acc_y  Acc_z  Gyro_x  Gyro_y  Gyro_z  \\\n",
       "  0        0.103896    0.032258   0.18  -1.71  11.40   -0.54    1.37    0.30   \n",
       "  1        0.103896    0.040323   0.15  -1.66  11.43   -0.53    1.33    0.36   \n",
       "  2        0.090909    0.040323   0.19  -1.58  11.45   -0.51    1.24    0.48   \n",
       "  3        0.090909    0.040323   0.33  -1.45  11.56   -0.51    1.13    0.61   \n",
       "  4        0.090909    0.040323   0.37  -1.48  11.44   -0.51    1.03    0.75   \n",
       "  5        0.103896    0.032258   0.46  -1.67  11.38   -0.52    0.93    0.84   \n",
       "  6        0.090909    0.040323   0.59  -2.06  11.41   -0.51    0.86    0.94   \n",
       "  7        0.090909    0.040323   0.61  -2.57  11.36   -0.47    0.84    1.01   \n",
       "  8        0.090909    0.040323   0.59  -3.01  11.23   -0.45    0.83    1.05   \n",
       "  9        0.090909    0.040323   0.52  -3.39  11.07   -0.40    0.85    1.05   \n",
       "  10       0.103896    0.040323   0.39  -3.55  10.99   -0.35    0.87    1.03   \n",
       "  11       0.116883    0.032258   0.31  -3.71  10.91   -0.29    0.91    1.01   \n",
       "  12       0.103896    0.040323   0.09  -3.68  10.75   -0.23    0.94    0.99   \n",
       "  13       0.090909    0.040323  -0.13  -3.65  10.65   -0.18    0.94    0.99   \n",
       "  14       0.103896    0.040323  -0.33  -3.60  10.56   -0.13    0.93    0.99   \n",
       "  15       0.103896    0.032258  -0.51  -3.60  10.55   -0.07    0.89    0.99   \n",
       "  16       0.103896    0.032258  -0.66  -3.66  10.39   -0.03    0.84    0.98   \n",
       "  17       0.090909    0.040323  -0.83  -3.58  10.30    0.01    0.80    0.97   \n",
       "  18       0.090909    0.040323  -1.02  -3.42  10.40    0.03    0.78    0.97   \n",
       "  19       0.090909    0.040323  -1.20  -3.21  10.63    0.04    0.78    0.97   \n",
       "  20       0.103896    0.040323  -1.33  -3.06  10.94    0.02    0.79    0.99   \n",
       "  21       0.090909    0.040323  -1.49  -2.96  11.06   -0.01    0.77    1.01   \n",
       "  22       0.077922    0.040323  -1.52  -2.91  11.15   -0.08    0.72    1.03   \n",
       "  23       0.090909    0.048387  -1.47  -2.93  10.98   -0.18    0.64    1.04   \n",
       "  24       0.103896    0.040323  -1.59  -3.09  10.53   -0.24    0.56    1.05   \n",
       "  25       0.090909    0.040323  -1.81  -3.41   9.98   -0.28    0.51    1.07   \n",
       "  26       0.090909    0.040323  -2.05  -3.86   9.44   -0.27    0.49    1.07   \n",
       "  27       0.090909    0.040323  -2.46  -4.27   9.16   -0.22    0.50    1.06   \n",
       "  28       0.090909    0.040323  -2.86  -4.53   8.94   -0.15    0.49    1.04   \n",
       "  29       0.090909    0.040323  -3.37  -4.60   8.71   -0.04    0.47    1.01   \n",
       "  ...           ...         ...    ...    ...    ...     ...     ...     ...   \n",
       "  24970    0.147059    0.522124   0.76  -0.66  10.00   -0.00    0.00    0.03   \n",
       "  24971    0.117647    0.513274   0.82  -0.66   9.98   -0.00    0.02    0.03   \n",
       "  24972    0.132353    0.513274   0.83  -0.64   9.96   -0.01    0.02    0.03   \n",
       "  24973    0.117647    0.504425   0.79  -0.71   9.97   -0.00    0.02    0.03   \n",
       "  24974    0.132353    0.486726   0.79  -0.66   9.99   -0.01    0.01    0.03   \n",
       "  24975    0.132353    0.486726   0.80  -0.68   9.96   -0.01    0.01    0.03   \n",
       "  24976    0.147059    0.486726   0.82  -0.66   9.98   -0.01    0.00    0.03   \n",
       "  24977    0.132353    0.486726   0.80  -0.68   9.97   -0.01    0.00    0.03   \n",
       "  24978    0.132353    0.495575   0.79  -0.67  10.01   -0.01    0.01    0.03   \n",
       "  24979    0.132353    0.495575   0.83  -0.68  10.00   -0.01    0.00    0.03   \n",
       "  24980    0.147059    0.495575   0.81  -0.63   9.95   -0.01    0.00    0.03   \n",
       "  24981    0.147059    0.495575   0.81  -0.62  10.00   -0.01    0.01    0.03   \n",
       "  24982    0.132353    0.486726   0.86  -0.65   9.98   -0.01    0.00    0.03   \n",
       "  24983    0.132353    0.486726   0.82  -0.67  10.00   -0.01    0.01    0.03   \n",
       "  24984    0.132353    0.486726   0.79  -0.66  10.00   -0.01    0.01    0.03   \n",
       "  24985    0.132353    0.477876   0.84  -0.64  10.02   -0.01    0.01    0.03   \n",
       "  24986    0.132353    0.477876   0.83  -0.69  10.04   -0.01    0.01    0.03   \n",
       "  24987    0.132353    0.477876   0.83  -0.67  10.04   -0.01    0.01    0.03   \n",
       "  24988    0.161765    0.477876   0.81  -0.64   9.99   -0.01    0.01    0.03   \n",
       "  24989    0.132353    0.469027   0.88  -0.64  10.01   -0.01    0.01    0.03   \n",
       "  24990    0.132353    0.469027   0.86  -0.69  10.01   -0.01    0.01    0.03   \n",
       "  24991    0.132353    0.460177   0.86  -0.67   9.98   -0.01    0.00    0.03   \n",
       "  24992    0.147059    0.460177   0.86  -0.65  10.00   -0.01    0.00    0.03   \n",
       "  24993    0.161765    0.451327   0.84  -0.69   9.97   -0.01   -0.00    0.03   \n",
       "  24994    0.147059    0.469027   0.86  -0.68   9.94   -0.01   -0.01    0.03   \n",
       "  24995    0.147059    0.469027   0.87  -0.69   9.98   -0.02   -0.02    0.02   \n",
       "  24996    0.132353    0.469027   0.86  -0.65   9.98   -0.02   -0.02    0.02   \n",
       "  24997    0.132353    0.460177   0.87  -0.67  10.02   -0.02   -0.03    0.02   \n",
       "  24998    0.102941    0.477876   0.86  -0.66  10.00   -0.02   -0.03    0.03   \n",
       "  24999    0.117647    0.469027   0.87  -0.68   9.97   -0.02   -0.03    0.02   \n",
       "  \n",
       "               Time  \n",
       "  0        0.017000  \n",
       "  1        0.023000  \n",
       "  2        0.029000  \n",
       "  3        0.035000  \n",
       "  4        0.041000  \n",
       "  5        0.048000  \n",
       "  6        0.054000  \n",
       "  7        0.060000  \n",
       "  8        0.066000  \n",
       "  9        0.072000  \n",
       "  10       0.078000  \n",
       "  11       0.084000  \n",
       "  12       0.091000  \n",
       "  13       0.097000  \n",
       "  14       0.103000  \n",
       "  15       0.110000  \n",
       "  16       0.116000  \n",
       "  17       0.122000  \n",
       "  18       0.129000  \n",
       "  19       0.135000  \n",
       "  20       0.141000  \n",
       "  21       0.147000  \n",
       "  22       0.153000  \n",
       "  23       0.159000  \n",
       "  24       0.165000  \n",
       "  25       0.172000  \n",
       "  26       0.178000  \n",
       "  27       0.185000  \n",
       "  28       0.191000  \n",
       "  29       0.197000  \n",
       "  ...           ...  \n",
       "  24970  158.356347  \n",
       "  24971  158.362347  \n",
       "  24972  158.368347  \n",
       "  24973  158.374347  \n",
       "  24974  158.381347  \n",
       "  24975  158.388347  \n",
       "  24976  158.394347  \n",
       "  24977  158.400347  \n",
       "  24978  158.406347  \n",
       "  24979  158.412347  \n",
       "  24980  158.419347  \n",
       "  24981  158.425347  \n",
       "  24982  158.432347  \n",
       "  24983  158.438347  \n",
       "  24984  158.444347  \n",
       "  24985  158.451347  \n",
       "  24986  158.457347  \n",
       "  24987  158.463347  \n",
       "  24988  158.469347  \n",
       "  24989  158.477347  \n",
       "  24990  158.483347  \n",
       "  24991  158.489347  \n",
       "  24992  158.495347  \n",
       "  24993  158.501347  \n",
       "  24994  158.508347  \n",
       "  24995  158.514347  \n",
       "  24996  158.521347  \n",
       "  24997  158.527347  \n",
       "  24998  158.533347  \n",
       "  24999  158.539347  \n",
       "  \n",
       "  [25000 rows x 9 columns]}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Data Augmentation: subsample data from combined data and add noise\n",
      "Finish Augmenting Data: Normal, Time Taken 11.925 sec\n",
      "Finish Augmenting Data: Side Walk, Time Taken 11.568 sec\n",
      "Finish Augmenting Data: Inward, Time Taken 11.653 sec\n",
      "Finish Augmenting Data: Line, Time Taken 12.278 sec\n",
      "Finish All Data Augmentation, Time Taken: 47.424 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_AUG = DataAugmentor(All_combined_data)\n",
    "aug_all_types = DATA_AUG.all_types\n",
    "All_aug_data = DATA_AUG.subSampleAll(out_length=400, num_draw=2500, std_percentile= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Info: \n",
      "[0.36126511 0.32776616 0.13238509]\n",
      "Rescale Info:\n",
      "Voltage_01: (1.0, 0.0)\n",
      "Voltage_02: (1.0, 0.0)\n",
      "Acc_x: (48.0, -78.47)\n",
      "Acc_y: (38.43, -77.24)\n",
      "Acc_z: (78.47, -29.72)\n",
      "Gyro_x: (4.28, -4.28)\n",
      "Gyro_y: (4.28, -4.28)\n",
      "Gyro_z: (4.28, -4.28)\n",
      "Component Range: \n",
      "[-0.62 -0.61 -0.51]\n",
      "[0.7  0.8  0.61]\n",
      "Feature Order: {'Voltage_01': 0, 'Voltage_02': 1, 'Acc_x': 2, 'Acc_y': 3, 'Acc_z': 4, 'Gyro_x': 5, 'Gyro_y': 6, 'Gyro_z': 7}\n"
     ]
    }
   ],
   "source": [
    "# put all raw trials into a single df to apply PCA\n",
    "list_all_raw_data = []\n",
    "for each_gen in All_combined_data:\n",
    "    for each_spec in All_combined_data[each_gen]:\n",
    "        list_all_raw_data.append(All_combined_data[each_gen][each_spec])\n",
    "all_trial_df = pd.concat(list_all_raw_data)\n",
    "\n",
    "PCA_all = RescalePCA()\n",
    "reduced_dim = 3\n",
    "PCA_all.processRescalePCA(all_trial_df, reduced_dim)\n",
    "print(PCA_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hist_bins = 20\n",
    "num_fft_features = 10\n",
    "cut_off = 100\n",
    "def GetAllFeatures(pca_data, component_range, num_hist_bins,\n",
    "                   time_data, cut_off, num_fft_bins):\n",
    "    return np.hstack((HistFeature(pca_data, component_range, num_hist_bins), \n",
    "                                         FFTFeature(pca_data,sample['Time'], cut_off, num_fft_bins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strat Feature Extraction: \n",
      "Normal Completed: Time Taken: 1.025 sec ...\n",
      "Side Walk Completed: Time Taken: 1.399 sec ...\n",
      "Inward Completed: Time Taken: 0.98 sec ...\n",
      "Line Completed: Time Taken: 1.032 sec ...\n",
      "All Feature Extractions Completed, Ready to Train Model\n"
     ]
    }
   ],
   "source": [
    "# combine all the trials data into HistFeatures and combined into a matrix for modeling training \n",
    "# buffer the feature matrix and label array\n",
    "num_train_samples = sum([len(All_aug_data[each_type]) for each_type in All_aug_data])\n",
    "train_matrix = np.zeros((num_train_samples, reduced_dim * num_hist_bins + reduced_dim * num_fft_features))\n",
    "train_labels = np.zeros(num_train_samples)\n",
    "# process each sample for the hist features\n",
    "idx = 0\n",
    "raw_mea_matrix = []\n",
    "print('Strat Feature Extraction: ')\n",
    "for each_type in All_aug_data:\n",
    "    cur_ini_time = time.time()\n",
    "    cur_samples = All_aug_data[each_type]\n",
    "    cur_type_idx = gait_types.index(each_type)\n",
    "    for sample in cur_samples:\n",
    "        raw_mea_matrix.append(sample[list(sample)[:-1]])\n",
    "        cur_pca = PCA_all.applyRescalePCA(sample)\n",
    "        train_matrix[idx,:] = GetAllFeatures(cur_pca, PCA_all.component_range, num_hist_bins,\n",
    "                                            sample['Time'], cut_off, num_fft_features)\n",
    "        train_labels[idx] = cur_type_idx\n",
    "        idx += 1\n",
    "    print(each_type + \" Completed: Time Taken: \" + str(round(time.time() - cur_ini_time, 3)) + ' sec ...')\n",
    "print('All Feature Extractions Completed, Ready to Train Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Trained, Time Taken: 0.016 sec\n",
      "SVM Model Trained, Time Taken: 27.789 sec\n",
      "GBT Model Trained, Time Taken: 1.766 sec\n"
     ]
    }
   ],
   "source": [
    "log_ini_time = time.time()\n",
    "# Test if logistc regrisson works \n",
    "test_log = linear_model.LogisticRegression()\n",
    "test_log.fit(train_matrix, train_labels)\n",
    "print('Logistic Regression Model Trained, Time Taken: ' + str(round(time.time() - log_ini_time, 3)) + ' sec')\n",
    "\n",
    "svm_ini_time = time.time()\n",
    "# Support vector machine  \n",
    "test_svc = NuSVC(probability=True, gamma=1/1000, kernel='linear')\n",
    "test_svc.fit(train_matrix, train_labels)\n",
    "print('SVM Model Trained, Time Taken: ' + str(round(time.time() - svm_ini_time, 3)) + ' sec')\n",
    "\n",
    "gbt_ini_time = time.time()\n",
    "# Gradient Boosting Ensemble\n",
    "test_gbt = ensemble.GradientBoostingClassifier()\n",
    "test_gbt.fit(train_matrix, train_labels)\n",
    "print('GBT Model Trained, Time Taken: ' + str(round(time.time() - gbt_ini_time, 3)) + ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ReadDataForTest(trial_name, PCA_process_obj, num_hist_bins, cut_off, num_fft_features):\n",
    "    test_case = ToPandasData('../Data_Collection/' + trial_name + '.txt')\n",
    "    test_in = PCA_all.applyRescalePCA(test_case)\n",
    "    test_fea = GetAllFeatures(test_in, PCA_process_obj.component_range, num_hist_bins, \n",
    "                             test_case['Time'], cut_off, num_fft_features)\n",
    "    return {'Trial_Name': trial_name,\n",
    "            'Raw Data': test_case, \n",
    "            'PCA Data': test_in, \n",
    "            'Features': test_fea}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisTrainVsTest(train_matrix, train_label, gait_types,\n",
    "                                test_case, test_case_name,\n",
    "                                vis_feature_trans = 0.07, num_same_type = 30):\n",
    "    all_colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "    plt_starts = {}\n",
    "    # map each gait type in the input with a color\n",
    "    # split the combined feature matrix into sections of different gait type by idxs \n",
    "    for idx in range(len(train_label)):\n",
    "        cur_gait_name = gait_types[int(train_label[idx])]\n",
    "        if cur_gait_name not in plt_starts: plt_starts[cur_gait_name] = idx\n",
    "    # plot the features with transparancy to help visulize the feature is like \n",
    "    for idx in range(len(gait_types)):\n",
    "        each_type = gait_types[idx]\n",
    "        plt_range = range(plt_starts[each_type], plt_starts[each_type] + num_same_type)\n",
    "        plt.figure(figsize=(15,5))\n",
    "        for plt_idx in plt_range:\n",
    "            plt.plot(train_matrix[plt_idx,:], all_colors[idx], alpha=vis_feature_trans)\n",
    "        plt.plot(train_matrix[plt_idx,:], all_colors[idx], label = each_type)\n",
    "        plt.plot(test_case, 'xkcd:gold', linewidth=3.0, label = test_case_name)\n",
    "        plt.legend()\n",
    "        plt.ylim(-0.1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log')\n",
    "for idx in range(len(all_test_data)):\n",
    "    cur_test_fea = all_test_data[idx]['Features']\n",
    "    VisPredResult(test_log.predict_proba([cur_test_fea])[0], gait_types, str(idx) + ' ' + \"Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(all_test_data)):\n",
    "    cur_test_fea = all_test_data[idx]['Features']\n",
    "    VisPredResult(test_svc.predict_proba([cur_test_fea])[0], gait_types, str(idx) + ' ' + \"SVM\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(all_test_data)):\n",
    "    cur_test_fea = all_test_data[idx]['Features']\n",
    "    VisPredResult(test_gbt.predict_proba([cur_test_fea])[0], gait_types, str(idx+1) + ' ' + \"GBT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KerasFormat(old_x, old_y):\n",
    "    '''k for keras'''\n",
    "    k_y = np_utils.to_categorical(old_y)\n",
    "    k_x = np.array([np.array(each) for each in old_x])\n",
    "    return k_x, k_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_x, k_y = KerasFormat(raw_mea_matrix, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(k_x, k_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8001, 4)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8001, 400, 8)\n",
      "(8001, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(30, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(gait_types), activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 801 samples\n",
      "Epoch 1/70\n",
      "7200/7200 [==============================] - 2s 258us/step - loss: 1.5076 - acc: 0.2608 - val_loss: 1.4209 - val_acc: 0.2797\n",
      "Epoch 2/70\n",
      "7200/7200 [==============================] - 1s 171us/step - loss: 1.4344 - acc: 0.2850 - val_loss: 1.3713 - val_acc: 0.3196\n",
      "Epoch 3/70\n",
      "7200/7200 [==============================] - 1s 165us/step - loss: 1.3854 - acc: 0.3269 - val_loss: 1.3325 - val_acc: 0.3820\n",
      "Epoch 4/70\n",
      "7200/7200 [==============================] - 1s 178us/step - loss: 1.3541 - acc: 0.3582 - val_loss: 1.2988 - val_acc: 0.4357\n",
      "Epoch 5/70\n",
      "7200/7200 [==============================] - 1s 177us/step - loss: 1.3320 - acc: 0.3778 - val_loss: 1.2745 - val_acc: 0.4607\n",
      "Epoch 6/70\n",
      "7200/7200 [==============================] - 1s 198us/step - loss: 1.3044 - acc: 0.4071 - val_loss: 1.2515 - val_acc: 0.4744\n",
      "Epoch 7/70\n",
      "7200/7200 [==============================] - 1s 171us/step - loss: 1.2785 - acc: 0.4274 - val_loss: 1.2274 - val_acc: 0.4981\n",
      "Epoch 8/70\n",
      "7200/7200 [==============================] - 1s 161us/step - loss: 1.2546 - acc: 0.4401 - val_loss: 1.2030 - val_acc: 0.5231\n",
      "Epoch 9/70\n",
      "7200/7200 [==============================] - 1s 165us/step - loss: 1.2302 - acc: 0.4693 - val_loss: 1.1790 - val_acc: 0.5381\n",
      "Epoch 10/70\n",
      "7200/7200 [==============================] - 1s 160us/step - loss: 1.2128 - acc: 0.4800 - val_loss: 1.1539 - val_acc: 0.5655\n",
      "Epoch 11/70\n",
      "7200/7200 [==============================] - 1s 200us/step - loss: 1.1856 - acc: 0.5003 - val_loss: 1.1248 - val_acc: 0.5805\n",
      "Epoch 12/70\n",
      "7200/7200 [==============================] - 1s 173us/step - loss: 1.1581 - acc: 0.5271 - val_loss: 1.0890 - val_acc: 0.6217\n",
      "Epoch 13/70\n",
      "7200/7200 [==============================] - 1s 165us/step - loss: 1.1234 - acc: 0.5572 - val_loss: 1.0530 - val_acc: 0.6479\n",
      "Epoch 14/70\n",
      "7200/7200 [==============================] - 1s 160us/step - loss: 1.0871 - acc: 0.5797 - val_loss: 1.0266 - val_acc: 0.6617\n",
      "Epoch 15/70\n",
      "7200/7200 [==============================] - 1s 160us/step - loss: 1.0524 - acc: 0.6067 - val_loss: 0.9731 - val_acc: 0.7029\n",
      "Epoch 16/70\n",
      "7200/7200 [==============================] - 1s 164us/step - loss: 1.0042 - acc: 0.6375 - val_loss: 0.9318 - val_acc: 0.7166\n",
      "Epoch 17/70\n",
      "7200/7200 [==============================] - 1s 163us/step - loss: 0.9594 - acc: 0.6633 - val_loss: 0.8917 - val_acc: 0.7378\n",
      "Epoch 18/70\n",
      "7200/7200 [==============================] - 1s 156us/step - loss: 0.9227 - acc: 0.6768 - val_loss: 0.8515 - val_acc: 0.7566\n",
      "Epoch 19/70\n",
      "7200/7200 [==============================] - 1s 146us/step - loss: 0.8806 - acc: 0.7054 - val_loss: 0.8153 - val_acc: 0.7765\n",
      "Epoch 20/70\n",
      "7200/7200 [==============================] - 1s 178us/step - loss: 0.8496 - acc: 0.7171 - val_loss: 0.8282 - val_acc: 0.7453\n",
      "Epoch 21/70\n",
      "7200/7200 [==============================] - 1s 169us/step - loss: 0.8484 - acc: 0.7076 - val_loss: 0.7630 - val_acc: 0.7978\n",
      "Epoch 22/70\n",
      "7200/7200 [==============================] - 1s 160us/step - loss: 0.8062 - acc: 0.7318 - val_loss: 0.7359 - val_acc: 0.8040\n",
      "Epoch 23/70\n",
      "7200/7200 [==============================] - 1s 160us/step - loss: 0.7671 - acc: 0.7496 - val_loss: 0.7086 - val_acc: 0.8027\n",
      "Epoch 24/70\n",
      "7200/7200 [==============================] - 1s 162us/step - loss: 0.7397 - acc: 0.7600 - val_loss: 0.6800 - val_acc: 0.8152\n",
      "Epoch 25/70\n",
      "7200/7200 [==============================] - 1s 161us/step - loss: 0.7187 - acc: 0.7757 - val_loss: 0.6587 - val_acc: 0.8277\n",
      "Epoch 26/70\n",
      "7200/7200 [==============================] - 1s 164us/step - loss: 0.6940 - acc: 0.7799 - val_loss: 0.6309 - val_acc: 0.8277\n",
      "Epoch 27/70\n",
      "7200/7200 [==============================] - 1s 159us/step - loss: 0.6661 - acc: 0.7988 - val_loss: 0.6045 - val_acc: 0.8377\n",
      "Epoch 28/70\n",
      "7200/7200 [==============================] - 1s 162us/step - loss: 0.6438 - acc: 0.7981 - val_loss: 0.5846 - val_acc: 0.8464\n",
      "Epoch 29/70\n",
      "7200/7200 [==============================] - 1s 162us/step - loss: 0.6199 - acc: 0.8032 - val_loss: 0.5594 - val_acc: 0.8477\n",
      "Epoch 30/70\n",
      "7200/7200 [==============================] - 1s 171us/step - loss: 0.5914 - acc: 0.8135 - val_loss: 0.5362 - val_acc: 0.8477\n",
      "Epoch 31/70\n",
      "7200/7200 [==============================] - 1s 168us/step - loss: 0.5787 - acc: 0.8099 - val_loss: 0.5195 - val_acc: 0.8577\n",
      "Epoch 32/70\n",
      "7200/7200 [==============================] - 1s 162us/step - loss: 0.5584 - acc: 0.8225 - val_loss: 0.4937 - val_acc: 0.8689\n",
      "Epoch 33/70\n",
      "7200/7200 [==============================] - 1s 165us/step - loss: 0.5277 - acc: 0.8321 - val_loss: 0.4719 - val_acc: 0.8702\n",
      "Epoch 34/70\n",
      "7200/7200 [==============================] - 1s 161us/step - loss: 0.5107 - acc: 0.8328 - val_loss: 0.4503 - val_acc: 0.8727\n",
      "Epoch 35/70\n",
      "7200/7200 [==============================] - 1s 160us/step - loss: 0.4868 - acc: 0.8443 - val_loss: 0.4328 - val_acc: 0.8727\n",
      "Epoch 36/70\n",
      "7200/7200 [==============================] - 1s 166us/step - loss: 0.4643 - acc: 0.8517 - val_loss: 0.4166 - val_acc: 0.8789\n",
      "Epoch 37/70\n",
      "7200/7200 [==============================] - 1s 178us/step - loss: 0.4484 - acc: 0.8553 - val_loss: 0.3903 - val_acc: 0.8914\n",
      "Epoch 38/70\n",
      "7200/7200 [==============================] - 1s 165us/step - loss: 0.4277 - acc: 0.8650 - val_loss: 0.3668 - val_acc: 0.9051\n",
      "Epoch 39/70\n",
      "7200/7200 [==============================] - 1s 157us/step - loss: 0.4029 - acc: 0.8762 - val_loss: 0.3646 - val_acc: 0.9089\n",
      "Epoch 40/70\n",
      "7200/7200 [==============================] - 1s 164us/step - loss: 0.3908 - acc: 0.8783 - val_loss: 0.3257 - val_acc: 0.9338\n",
      "Epoch 41/70\n",
      "7200/7200 [==============================] - 1s 166us/step - loss: 0.3576 - acc: 0.8940 - val_loss: 0.3032 - val_acc: 0.9438\n",
      "Epoch 42/70\n",
      "7200/7200 [==============================] - 1s 170us/step - loss: 0.3399 - acc: 0.9081 - val_loss: 0.2809 - val_acc: 0.9526\n",
      "Epoch 43/70\n",
      "7200/7200 [==============================] - 1s 162us/step - loss: 0.3207 - acc: 0.9261 - val_loss: 0.2662 - val_acc: 0.9551\n",
      "Epoch 44/70\n",
      "7200/7200 [==============================] - 1s 162us/step - loss: 0.3009 - acc: 0.9357 - val_loss: 0.2471 - val_acc: 0.9675\n",
      "Epoch 45/70\n",
      "7200/7200 [==============================] - 1s 162us/step - loss: 0.2873 - acc: 0.9394 - val_loss: 0.2342 - val_acc: 0.9713\n",
      "Epoch 46/70\n",
      "7200/7200 [==============================] - 1s 166us/step - loss: 0.2758 - acc: 0.9461 - val_loss: 0.2265 - val_acc: 0.9688\n",
      "Epoch 47/70\n",
      "7200/7200 [==============================] - 1s 161us/step - loss: 0.2632 - acc: 0.9511 - val_loss: 0.2088 - val_acc: 0.9800\n",
      "Epoch 48/70\n",
      "7200/7200 [==============================] - 1s 160us/step - loss: 0.2450 - acc: 0.9590 - val_loss: 0.1973 - val_acc: 0.9800\n",
      "Epoch 49/70\n",
      "7200/7200 [==============================] - 1s 163us/step - loss: 0.2380 - acc: 0.9596 - val_loss: 0.1886 - val_acc: 0.9813\n",
      "Epoch 50/70\n",
      "7200/7200 [==============================] - 1s 161us/step - loss: 0.2251 - acc: 0.9687 - val_loss: 0.1773 - val_acc: 0.9838\n",
      "Epoch 51/70\n",
      "7200/7200 [==============================] - 1s 162us/step - loss: 0.2135 - acc: 0.9712 - val_loss: 0.1664 - val_acc: 0.9863\n",
      "Epoch 52/70\n",
      "7200/7200 [==============================] - 1s 163us/step - loss: 0.1992 - acc: 0.9743 - val_loss: 0.1583 - val_acc: 0.9850\n",
      "Epoch 53/70\n",
      "7200/7200 [==============================] - 1s 175us/step - loss: 0.2042 - acc: 0.9700 - val_loss: 0.1558 - val_acc: 0.9863\n",
      "Epoch 54/70\n",
      "7200/7200 [==============================] - 1s 177us/step - loss: 0.1912 - acc: 0.9764 - val_loss: 0.1428 - val_acc: 0.9888\n",
      "Epoch 55/70\n",
      "7200/7200 [==============================] - 1s 168us/step - loss: 0.1778 - acc: 0.9803 - val_loss: 0.1316 - val_acc: 0.9900\n",
      "Epoch 56/70\n",
      "7200/7200 [==============================] - 1s 161us/step - loss: 0.1697 - acc: 0.9824 - val_loss: 0.1224 - val_acc: 0.9925\n",
      "Epoch 57/70\n",
      "7200/7200 [==============================] - 1s 170us/step - loss: 0.1565 - acc: 0.9861 - val_loss: 0.1308 - val_acc: 0.9875\n",
      "Epoch 58/70\n",
      "7200/7200 [==============================] - 1s 187us/step - loss: 0.1563 - acc: 0.9815 - val_loss: 0.1090 - val_acc: 0.9913\n",
      "Epoch 59/70\n",
      "7200/7200 [==============================] - 1s 173us/step - loss: 0.1380 - acc: 0.9883 - val_loss: 0.1041 - val_acc: 0.9913\n",
      "Epoch 60/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 1s 190us/step - loss: 0.1339 - acc: 0.9882 - val_loss: 0.0939 - val_acc: 0.9938\n",
      "Epoch 61/70\n",
      "7200/7200 [==============================] - 1s 168us/step - loss: 0.1236 - acc: 0.9897 - val_loss: 0.0901 - val_acc: 0.9925\n",
      "Epoch 62/70\n",
      "7200/7200 [==============================] - 1s 165us/step - loss: 0.1197 - acc: 0.9897 - val_loss: 0.0793 - val_acc: 0.9963\n",
      "Epoch 63/70\n",
      "7200/7200 [==============================] - 1s 170us/step - loss: 0.1153 - acc: 0.9896 - val_loss: 0.0747 - val_acc: 0.9963\n",
      "Epoch 64/70\n",
      "7200/7200 [==============================] - 1s 168us/step - loss: 0.1086 - acc: 0.9908 - val_loss: 0.0803 - val_acc: 0.9938\n",
      "Epoch 65/70\n",
      "7200/7200 [==============================] - 1s 171us/step - loss: 0.1121 - acc: 0.9878 - val_loss: 0.0780 - val_acc: 0.9913\n",
      "Epoch 66/70\n",
      "7200/7200 [==============================] - 1s 182us/step - loss: 0.0984 - acc: 0.9912 - val_loss: 0.0695 - val_acc: 0.9938\n",
      "Epoch 67/70\n",
      "7200/7200 [==============================] - 1s 179us/step - loss: 0.0956 - acc: 0.9925 - val_loss: 0.0654 - val_acc: 0.9963\n",
      "Epoch 68/70\n",
      "7200/7200 [==============================] - 1s 196us/step - loss: 0.0915 - acc: 0.9933 - val_loss: 0.0664 - val_acc: 0.9925\n",
      "Epoch 69/70\n",
      "7200/7200 [==============================] - 1s 196us/step - loss: 0.0824 - acc: 0.9935 - val_loss: 0.0600 - val_acc: 0.9938\n",
      "Epoch 70/70\n",
      "7200/7200 [==============================] - 1s 167us/step - loss: 0.0810 - acc: 0.9940 - val_loss: 0.0613 - val_acc: 0.9925\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, batch_size= 4096, epochs=70, validation_split = 0.1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = hist.validation_data[0]\n",
    "valid_y = hist.validation_data[1]\n",
    "valid_pred = model.predict(valid_x, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996287845864117\n",
      "0.9997851204152135\n",
      "0.9999164578111946\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "for idx in range(4):\n",
    "    print(roc_auc_score(valid_y[:,idx], valid_pred[:,idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = []\n",
    "test_trials_type = 'ran_2'\n",
    "for idx in range(1,5):\n",
    "    cur_case_name = test_trials_type + '_0' + str(idx)\n",
    "    cur_test_case = ReadDataForTest(cur_case_name, PCA_all, num_hist_bins, cut_off, num_fft_features)\n",
    "    all_test_data.append(cur_test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8cda2e99e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c65af4080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c65ae8b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c65a22748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in range(len(all_test_data)):\n",
    "    test_case = all_test_data[1]['Raw Data']\n",
    "    sum_prob = np.zeros((1,4))\n",
    "    for idx in range(50,1000,25):\n",
    "        temp_case = test_case[list(test_case)[:-1]][idx: 400 + idx]\n",
    "        GetAllFeatures()\n",
    "        sum_prob += model.predict_proba(np.array(temp_case).reshape(1,400,8))[0]\n",
    "    VisPredResult((sum_prob/np.sum(sum_prob))[0], gait_types,\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01160464, 0.46128948, 0.35424441, 0.17286148])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum_prob/np.sum(sum_prob))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
